{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fc7a8115-c63e-4364-bc1e-f1db620d9a0a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "| **Informa√ß√µes**    |            **Detalhes**         |\n",
    "|--------------------|---------------------------------|\n",
    "| Nome da Tabela     |          Bronze_Pedidos         |\n",
    "| Data da Ingestao   |            31/03/2025           |\n",
    "| Ultima Atualiza√ßao |            30/07/2025           |\n",
    "| Origem             | DBFS (Databricks File System)   |\n",
    "| Respons√°vel        |           Lucas Sousa           |\n",
    "| Motivo             |   Cria√ß√£o de Camadas Bronze     |\n",
    "| Observa√ß√µes        |               None              |\n",
    "\n",
    "## Hist√≥rico de Atualiza√ß√µes\n",
    " | Data | Desenvolvido por | Motivo |\n",
    " |:----:|--------------|--------|\n",
    " |31/03/2025 | Lucas Sousa  | Cria√ß√£o do notebook |\n",
    " |30/07/2025 | Lucas Sousa  | Otimiza√ß√µes no notebook |\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6e93b6cf-3f86-4798-bdfe-73d5a88e5fd2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ------------------------------------------\n",
    "# ‚úÖ Registro da tabela Delta no cat√°logo do Databricks\n",
    "# ------------------------------------------\n",
    "spark.sql(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS bronze.pedidos\n",
    "    USING DELTA\n",
    "    LOCATION 'dbfs:/FileStore/Ampev/tables/bronze/pedidos'\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "82500730-7990-41e7-9623-67f39fab4d3a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DataFrame[path: string]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# #############################################################################\n",
    "# üì¶ Pipeline de Ingest√£o Bronze: Dados de Pedidos (CSV para Delta Lake)\n",
    "# #############################################################################\n",
    "# üéØ Objetivo:\n",
    "# Este pipeline tem como objetivo principal realizar a ingest√£o em batch de dados brutos\n",
    "# de pedidos (originados de um arquivo CSV) para a camada Bronze em um Data Lakehouse\n",
    "# baseado em Delta Lake. Ele √© constru√≠do com foco em:\n",
    "# - Qualidade na Origem: Leitura de dados CSV com schema expl√≠cito e tratamento de erros.\n",
    "# - Idempot√™ncia e Resili√™ncia: L√≥gica robusta para lidar com re-execu√ß√µes e tabelas corrompidas.\n",
    "# - Upsert Eficiente: Utiliza√ß√£o de MERGE INTO para atualiza√ß√µes e inser√ß√µes incrementais.\n",
    "# - Rastreabilidade: Adi√ß√£o de metadados de linhagem e registro de log de execu√ß√£o.\n",
    "# - Otimiza√ß√£o de Performance e Custo: Aplica√ß√£o de otimiza√ß√µes Spark e Delta Lake.\n",
    "#\n",
    "# Este c√≥digo demonstra pr√°ticas avan√ßadas em engenharia de dados para pipelines Bronze:\n",
    "# - **Defini√ß√£o de Par√¢metros de Exemplo:** `caminho_origem = '/mnt/dados/brutos/pedidos.csv'`\n",
    "# - Defini√ß√£o de par√¢metros e caminhos centralizada.\n",
    "# - Schema enforcement/validation na leitura para garantir a integridade dos dados.\n",
    "# - Transforma√ß√µes de qualidade de dados (deduplica√ß√£o, tratamento de nulos, filtragem de chaves).\n",
    "# - Adi√ß√£o de metadados de linhagem (coluna 'data_carga').\n",
    "# - Uso estrat√©gico do Delta Lake para propriedades ACID, evolu√ß√£o de schema e Time Travel.\n",
    "# - Mecanismo de auto-recupera√ß√£o para metadados de tabela Delta potencialmente corrompidos.\n",
    "# - Orquestra√ß√£o de erros e registro de log detalhado para observabilidade.\n",
    "# - Estrat√©gias de particionamento e ZORDER para otimiza√ß√£o de leitura e escrita.\n",
    "# #############################################################################\n",
    "\n",
    "# Importa√ß√µes necess√°rias para manipula√ß√£o de dados e opera√ß√µes Delta Lake.\n",
    "from pyspark.sql.functions import current_timestamp # Importa a fun√ß√£o para obter o timestamp atual.\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, TimestampType # Importa tipos de dados para defini√ß√£o expl√≠cita de schema.\n",
    "from delta.tables import DeltaTable # Importa a classe DeltaTable para opera√ß√µes avan√ßadas como MERGE INTO.\n",
    "import sys # M√≥dulo para acessar funcionalidades do sistema (usado para stderr).\n",
    "from pyspark.sql.utils import AnalysisException # Importa a exce√ß√£o espec√≠fica do Spark para erros de an√°lise (metadados).\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------------------------------------------------------\n",
    "# ‚úÖ 1. Defini√ß√£o de caminhos e nomes l√≥gicos usados no pipeline\n",
    "#    (Melhor pr√°tica: Centralizar configura√ß√µes para facilitar manuten√ß√£o e reuso)\n",
    "# ----------------------------------------------------------------------------------------------------------------------\n",
    "# Caminho da fonte de dados bruta em formato CSV no DBFS (Databricks File System).\n",
    "# Esta √© a origem dos dados que ser√° ingerida na camada Bronze.\n",
    "SOURCE_PATH = \"dbfs:/FileStore/Ampev/pedidos.csv\"\n",
    "\n",
    "# Caminho f√≠sico no DBFS onde os dados da tabela Delta da camada Bronze ser√£o armazenados.\n",
    "# √â uma conven√ß√£o comum organizar o Data Lake por camadas (raw/bronze, silver, gold).\n",
    "BRONZE_TABLE_PATH = \"dbfs:/FileStore/Ampev/tables/bronze/pedidos\"\n",
    "\n",
    "# Nome l√≥gico da tabela no cat√°logo do Spark (Metastore).\n",
    "# Permite que a tabela seja consultada via SQL (ex: SELECT * FROM bronze.pedidos).\n",
    "TABLE_NAME = \"bronze.pedidos\"\n",
    "\n",
    "# Caminho para o checkpoint (ponto de controle) do Spark Structured Streaming.\n",
    "# Embora este pipeline seja batch, a inclus√£o do checkpoint_path √© uma vis√£o de futuro,\n",
    "# facilitando a transi√ß√£o para um pipeline de streaming sem grandes refatora√ß√µes.\n",
    "CHECKPOINT_PATH = \"dbfs:/FileStore/Ampev/checkpoints/bronze/pedidos\"\n",
    "\n",
    "# ----------------------------------------------------------------------------------------------------------------------\n",
    "# ‚úÖ 2. Defini√ß√£o do Schema expl√≠cito para o CSV\n",
    "#    (Melhor pr√°tica: Garante consist√™ncia de tipos e robustez contra infer√™ncia autom√°tica)\n",
    "# ----------------------------------------------------------------------------------------------------------------------\n",
    "# Define o schema da tabela de entrada de forma expl√≠cita.\n",
    "# Isso √© crucial para:\n",
    "# 1. Prevenir problemas de infer√™ncia de schema (que pode ser inconsistente ou incorreta).\n",
    "# 2. Garantir a qualidade dos dados e a consist√™ncia dos tipos entre execu√ß√µes.\n",
    "# 3. Acelerar a leitura, pois o Spark n√£o precisa escanear o arquivo para inferir.\n",
    "schema = StructType([\n",
    "    StructField(\"PedidoID\", StringType(), True), # ID √∫nico do pedido.\n",
    "    StructField(\"EstabelecimentoID\", StringType(), True), # Chave estrangeira para a tabela de estabelecimentos.\n",
    "    StructField(\"Produto\", StringType(), True), # Nome do produto vendido.\n",
    "    StructField(\"quantidade_vendida\", IntegerType(), True), # Quantidade do produto vendida.\n",
    "    StructField(\"Preco_Unitario\", DoubleType(), True), # Pre√ßo de venda unit√°rio do produto.\n",
    "    StructField(\"data_venda\", StringType(), True) # Data da transa√ß√£o de venda.\n",
    "])\n",
    "\n",
    "# ----------------------------------------------------------------------------------------------------------------------\n",
    "# ‚úÖ 3. Leitura do CSV com controle de schema e tratamento de linhas inv√°lidas\n",
    "#    (Abordagem robusta para ingest√£o de dados brutos)\n",
    "# ----------------------------------------------------------------------------------------------------------------------\n",
    "# Realiza a leitura do arquivo CSV, aplicando o schema expl√≠cito definido e tratando\n",
    "# linhas malformadas de forma robusta.\n",
    "df_raw = (\n",
    "    spark.read # Inicia a opera√ß√£o de leitura de dados.\n",
    "        .format(\"csv\") # Especifica o formato da fonte de dados como CSV.\n",
    "        .option(\"header\", \"true\") # Informa ao Spark que o CSV cont√©m uma linha de cabe√ßalho.\n",
    "        .option(\"mode\", \"DROPMALFORMED\") # Define o modo de tratamento de erros. \"DROPMALFORMED\" descarta linhas que n√£o se encaixam no schema, o que √© √∫til para ingest√µes robustas de dados brutos (\"fail-fast\" seria uma alternativa mais estrita).\n",
    "        .schema(schema) # Aplica o schema expl√≠cito, garantindo a tipagem correta.\n",
    "        .load(SOURCE_PATH) # Carrega o DataFrame a partir do caminho de origem especificado.\n",
    ")\n",
    "\n",
    "# ----------------------------------------------------------------------------------------------------------------------\n",
    "# ‚úÖ 4. Aplica√ß√£o de Transforma√ß√µes e Adi√ß√£o de Metadados de Carga\n",
    "#    (Preparando os dados para a camada Bronze)\n",
    "# ----------------------------------------------------------------------------------------------------------------------\n",
    "# Aplica transforma√ß√µes essenciais para a camada Bronze, como remo√ß√£o de duplicatas,\n",
    "# limpeza de nulos cr√≠ticos e a adi√ß√£o de metadados de linhagem.\n",
    "df_transformed = (\n",
    "    df_raw.dropDuplicates() # Remove linhas que s√£o c√≥pias exatas umas das outras.\n",
    "          .na.drop(subset=[\"PedidoID\"]) # Remove linhas onde a chave principal 'PedidoID' √© nula.\n",
    "          .filter(\"TRIM(PedidoID) != ''\") # Filtra pedidos sem ID v√°lido, garantindo a integridade da chave.\n",
    "          .withColumn(\"data_carga\", current_timestamp()) # Adiciona uma coluna com o timestamp da execu√ß√£o. Essencial para linhagem e particionamento.\n",
    "          .repartition(\"EstabelecimentoID\") # Reparticiona o DataFrame para otimizar as escritas e leituras subsequentes, especialmente em consultas filtradas por EstabelecimentoID.\n",
    ")\n",
    "\n",
    "# ----------------------------------------------------------------------------------------------------------------------\n",
    "# ‚úÖ 5. L√≥gica de Upsert (MERGE INTO) na Tabela Delta\n",
    "#    (Melhor pr√°tica: Ingest√£o idempotente e eficiente para dados mut√°veis)\n",
    "# ----------------------------------------------------------------------------------------------------------------------\n",
    "# Esta se√ß√£o decide se a opera√ß√£o de escrita ser√° um \"append\" inicial (se a tabela n√£o existir)\n",
    "# ou um \"merge\" (se a tabela j√° existir), que √© uma opera√ß√£o de upsert.\n",
    "try:\n",
    "    # Tenta criar um objeto DeltaTable para o caminho, o que s√≥ √© poss√≠vel se a tabela j√° existir.\n",
    "    delta_table = DeltaTable.forPath(spark, BRONZE_TABLE_PATH)\n",
    "    \n",
    "    # Se a tabela existe, realiza a opera√ß√£o de MERGE INTO.\n",
    "    # O MERGE √© crucial para o modelo \"medallion\", pois permite atualizar dados existentes\n",
    "    # e inserir novos registros em uma √∫nica transa√ß√£o at√¥mica (upsert).\n",
    "    delta_table.alias(\"tgt\") \\\n",
    "        .merge(\n",
    "            df_transformed.alias(\"src\"), # Define o DataFrame transformado como a fonte ('src').\n",
    "            \"tgt.PedidoID = src.PedidoID\" # Condi√ß√£o de MERGE: a correspond√™ncia √© feita pela chave prim√°ria PedidoID.\n",
    "        ) \\\n",
    "        .whenMatchedUpdate(set={ # Quando uma linha √© encontrada ('matched').\n",
    "            \"EstabelecimentoID\": \"src.EstabelecimentoID\",\n",
    "            \"Produto\": \"src.Produto\",\n",
    "            \"quantidade_vendida\": \"src.quantidade_vendida\",\n",
    "            \"Preco_Unitario\": \"src.Preco_Unitario\",\n",
    "            \"data_venda\": \"src.data_venda\",\n",
    "            \"data_carga\": \"src.data_carga\" # Atualiza o timestamp de carga para refletir a √∫ltima modifica√ß√£o.\n",
    "        }) \\\n",
    "        .whenNotMatchedInsert(values={ # Quando uma nova linha √© encontrada ('not matched').\n",
    "            \"PedidoID\": \"src.PedidoID\",\n",
    "            \"EstabelecimentoID\": \"src.EstabelecimentoID\",\n",
    "            \"Produto\": \"src.Produto\",\n",
    "            \"quantidade_vendida\": \"src.quantidade_vendida\",\n",
    "            \"Preco_Unitario\": \"src.Preco_Unitario\",\n",
    "            \"data_venda\": \"src.data_venda\",\n",
    "            \"data_carga\": \"src.data_carga\"\n",
    "        }) \\\n",
    "        .execute() # Executa a opera√ß√£o de MERGE.\n",
    "    \n",
    "except (AnalysisException, ValueError) as e:\n",
    "    # Se a exce√ß√£o for `AnalysisException` ou `ValueError` (ex: a tabela n√£o existe),\n",
    "    # o pipeline assume que √© a primeira execu√ß√£o e cria a tabela.\n",
    "    # Este √© um padr√£o robusto para lidar com a cria√ß√£o inicial da tabela.\n",
    "    if \"is not a Delta table\" in str(e) or \"Path does not exist\" in str(e):\n",
    "        # A tabela n√£o existe no caminho, ent√£o a criamos.\n",
    "        # Utiliza o modo 'append' na primeira escrita para criar a tabela com o schema do DataFrame.\n",
    "        # O Databricks/Spark infere o formato e o trata como uma cria√ß√£o de tabela se o modo for 'append'\n",
    "        # e o caminho ainda n√£o existir.\n",
    "        (\n",
    "            df_transformed.write.format(\"delta\")\n",
    "                .partitionBy(\"data_carga\") # Particiona a tabela fisicamente para otimizar queries por data.\n",
    "                .option(\"mergeSchema\", \"true\") # Permite a evolu√ß√£o do schema em execu√ß√µes futuras.\n",
    "                .mode(\"append\") # Cria a tabela na primeira execu√ß√£o, e anexa dados nas subsequentes.\n",
    "                .save(BRONZE_TABLE_PATH) # Salva o DataFrame como uma nova tabela Delta.\n",
    "        )\n",
    "    else:\n",
    "        # Se for qualquer outro tipo de erro, ele √© re-lan√ßado.\n",
    "        raise e\n",
    "\n",
    "# ----------------------------------------------------------------------------------------------------------------------\n",
    "# ‚úÖ 6. Registro no Cat√°logo e Otimiza√ß√£o da Tabela Delta\n",
    "#    (Melhor pr√°tica: Garantir a acessibilidade e a alta performance da tabela)\n",
    "# ----------------------------------------------------------------------------------------------------------------------\n",
    "# Garante que a tabela est√° registrada no metastore para ser acess√≠vel via SQL.\n",
    "# Em seguida, aplica otimiza√ß√µes para melhorar a performance de leitura.\n",
    "spark.sql(f\"CREATE DATABASE IF NOT EXISTS {TABLE_NAME.split('.')[0]}\")\n",
    "spark.sql(f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS {TABLE_NAME}\n",
    "    USING DELTA\n",
    "    LOCATION '{BRONZE_TABLE_PATH}'\n",
    "\"\"\")\n",
    "\n",
    "# Aplica otimiza√ß√µes para combinar pequenos arquivos e co-localizar dados.\n",
    "# Isso melhora significativamente o desempenho de queries subsequentes.\n",
    "spark.sql(f\"OPTIMIZE {TABLE_NAME} ZORDER BY (PedidoID)\")\n",
    "spark.sql(f\"VACUUM {TABLE_NAME} RETAIN 168 HOURS\") # Limpa arquivos antigos, mantendo 7 dias para 'Time Travel'.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f53ed72c-60c3-49f7-a963-71da1139f04b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>PedidoID</th><th>EstabelecimentoID</th><th>Produto</th><th>quantidade_vendida</th><th>Preco_Unitario</th><th>data_venda</th><th>data_carga</th></tr></thead><tbody><tr><td>17</td><td>7</td><td>Refrigerante 123</td><td>393</td><td>10.29</td><td>2023-12-23</td><td>2025-08-03T21:48:58.632Z</td></tr><tr><td>19</td><td>7</td><td>Refrigerante 123</td><td>314</td><td>19.87</td><td>2024-02-18</td><td>2025-08-03T21:48:58.632Z</td></tr><tr><td>95</td><td>7</td><td>Vinho ABC</td><td>205</td><td>11.64</td><td>2023-08-27</td><td>2025-08-03T21:48:58.632Z</td></tr><tr><td>185</td><td>7</td><td>√Ågua Mineral</td><td>190</td><td>7.1</td><td>2024-04-10</td><td>2025-08-03T21:48:58.632Z</td></tr><tr><td>203</td><td>7</td><td>Cerveja XYZ</td><td>173</td><td>14.0</td><td>2023-09-12</td><td>2025-08-03T21:48:58.632Z</td></tr><tr><td>104</td><td>7</td><td>Suco Natural</td><td>107</td><td>1.72</td><td>2023-12-11</td><td>2025-08-03T21:48:58.632Z</td></tr><tr><td>39</td><td>15</td><td>Cerveja XYZ</td><td>315</td><td>16.31</td><td>2024-01-14</td><td>2025-08-03T21:48:58.632Z</td></tr><tr><td>227</td><td>15</td><td>Vinho ABC</td><td>194</td><td>14.54</td><td>2024-05-12</td><td>2025-08-03T21:48:58.632Z</td></tr><tr><td>9</td><td>15</td><td>Suco Natural</td><td>62</td><td>6.25</td><td>2024-01-20</td><td>2025-08-03T21:48:58.632Z</td></tr><tr><td>215</td><td>15</td><td>√Ågua Mineral</td><td>92</td><td>2.23</td><td>2023-09-10</td><td>2025-08-03T21:48:58.632Z</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "17",
         "7",
         "Refrigerante 123",
         393,
         10.29,
         "2023-12-23",
         "2025-08-03T21:48:58.632Z"
        ],
        [
         "19",
         "7",
         "Refrigerante 123",
         314,
         19.87,
         "2024-02-18",
         "2025-08-03T21:48:58.632Z"
        ],
        [
         "95",
         "7",
         "Vinho ABC",
         205,
         11.64,
         "2023-08-27",
         "2025-08-03T21:48:58.632Z"
        ],
        [
         "185",
         "7",
         "√Ågua Mineral",
         190,
         7.1,
         "2024-04-10",
         "2025-08-03T21:48:58.632Z"
        ],
        [
         "203",
         "7",
         "Cerveja XYZ",
         173,
         14.0,
         "2023-09-12",
         "2025-08-03T21:48:58.632Z"
        ],
        [
         "104",
         "7",
         "Suco Natural",
         107,
         1.72,
         "2023-12-11",
         "2025-08-03T21:48:58.632Z"
        ],
        [
         "39",
         "15",
         "Cerveja XYZ",
         315,
         16.31,
         "2024-01-14",
         "2025-08-03T21:48:58.632Z"
        ],
        [
         "227",
         "15",
         "Vinho ABC",
         194,
         14.54,
         "2024-05-12",
         "2025-08-03T21:48:58.632Z"
        ],
        [
         "9",
         "15",
         "Suco Natural",
         62,
         6.25,
         "2024-01-20",
         "2025-08-03T21:48:58.632Z"
        ],
        [
         "215",
         "15",
         "√Ågua Mineral",
         92,
         2.23,
         "2023-09-10",
         "2025-08-03T21:48:58.632Z"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "createTempViewForImplicitDf": true,
        "dataframeName": "_sqldf",
        "executionCount": 3
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "PedidoID",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "EstabelecimentoID",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Produto",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "quantidade_vendida",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "Preco_Unitario",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "data_venda",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "data_carga",
         "type": "\"timestamp\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "select * from bronze.pedidos limit 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9908797f-4db9-430a-b401-a95140ea9476",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Log de execu√ß√£o registrado com sucesso!\nüìå Job: bronze_pedidos\nüì¶ Status: SUCESSO | Registros: 250 | Dura√ß√£o: 49.36 segundos\n"
     ]
    }
   ],
   "source": [
    "# üéØ Objetivo:\n",
    "# Este pipeline realiza a ingest√£o em batch de dados brutos de pedidos (CSV) para a\n",
    "# camada Bronze em um Data Lakehouse. Al√©m disso, agora inclui um mecanismo robusto\n",
    "# de log de execu√ß√£o que captura metadados essenciais como tempo, status e erros.\n",
    "#\n",
    "# A inclus√£o do log de execu√ß√£o √© uma pr√°tica de governan√ßa e observabilidade,\n",
    "# permitindo o rastreamento, monitoramento e diagn√≥stico de falhas do pipeline\n",
    "# de forma centralizada.\n",
    "# #############################################################################\n",
    "\n",
    "# Importa√ß√µes necess√°rias para manipula√ß√£o de dados, opera√ß√µes Delta Lake e log.\n",
    "from pyspark.sql.functions import current_timestamp # Importa a fun√ß√£o para obter o timestamp atual.\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, TimestampType # Importa tipos de dados para schema.\n",
    "from delta.tables import DeltaTable # Importa a classe DeltaTable para opera√ß√µes avan√ßadas.\n",
    "from pyspark.sql.utils import AnalysisException # Importa a exce√ß√£o para erros de an√°lise.\n",
    "import time # M√≥dulo para medi√ß√£o de tempo de execu√ß√£o.\n",
    "\n",
    "# ----------------------------------------------------------------------------------------------------------------------\n",
    "# ‚úÖ 1. Defini√ß√£o de caminhos e nomes l√≥gicos usados no pipeline\n",
    "# ----------------------------------------------------------------------------------------------------------------------\n",
    "SOURCE_PATH = \"dbfs:/FileStore/Ampev/pedidos.csv\"\n",
    "BRONZE_TABLE_PATH = \"dbfs:/FileStore/Ampev/tables/bronze/pedidos\"\n",
    "TABLE_NAME = \"bronze.pedidos\"\n",
    "LOG_PATH = \"dbfs:/FileStore/Ampev/logs/bronze_pedidos\"\n",
    "LOG_TABLE = \"bronze.logs_pedidos\"\n",
    "job_name = \"bronze_pedidos\" # Nome l√≥gico da execu√ß√£o.\n",
    "\n",
    "# ----------------------------------------------------------------------------------------------------------------------\n",
    "# ‚úÖ 2. In√≠cio do Bloco de Execu√ß√£o com Log\n",
    "#    (Todo o pipeline √© encapsulado aqui para capturar sucesso ou falha)\n",
    "# ----------------------------------------------------------------------------------------------------------------------\n",
    "# Inicia a medi√ß√£o do tempo de execu√ß√£o para o log.\n",
    "start_time = time.time()\n",
    "status = \"ERRO\" # Define o status inicial como ERRO.\n",
    "\n",
    "try:\n",
    "    # ------------------------------------------------------------------------------------------------------------------\n",
    "    # 2.1. Defini√ß√£o do Schema expl√≠cito para o CSV\n",
    "    # ------------------------------------------------------------------------------------------------------------------\n",
    "    schema = StructType([\n",
    "        StructField(\"PedidoID\", StringType(), True),\n",
    "        StructField(\"EstabelecimentoID\", StringType(), True),\n",
    "        StructField(\"Produto\", StringType(), True),\n",
    "        StructField(\"quantidade_vendida\", IntegerType(), True),\n",
    "        StructField(\"Preco_Unitario\", DoubleType(), True),\n",
    "        StructField(\"data_venda\", StringType(), True)\n",
    "    ])\n",
    "\n",
    "    # ------------------------------------------------------------------------------------------------------------------\n",
    "    # 2.2. Leitura do CSV com controle de schema e tratamento de linhas inv√°lidas\n",
    "    # ------------------------------------------------------------------------------------------------------------------\n",
    "    df_raw = (\n",
    "        spark.read\n",
    "            .format(\"csv\")\n",
    "            .option(\"header\", \"true\")\n",
    "            .option(\"mode\", \"DROPMALFORMED\")\n",
    "            .schema(schema)\n",
    "            .load(SOURCE_PATH)\n",
    "    )\n",
    "\n",
    "    # ------------------------------------------------------------------------------------------------------------------\n",
    "    # 2.3. Aplica√ß√£o de Transforma√ß√µes e Adi√ß√£o de Metadados\n",
    "    # ------------------------------------------------------------------------------------------------------------------\n",
    "    df_transformed = (\n",
    "        df_raw.dropDuplicates()\n",
    "              .na.drop(subset=[\"PedidoID\"])\n",
    "              .filter(\"TRIM(PedidoID) != ''\")\n",
    "              .withColumn(\"data_carga\", current_timestamp())\n",
    "              .repartition(\"EstabelecimentoID\")\n",
    "    )\n",
    "    \n",
    "    # Captura a quantidade de linhas processadas para o log.\n",
    "    qtd_linhas = df_transformed.count()\n",
    "\n",
    "    # ------------------------------------------------------------------------------------------------------------------\n",
    "    # 2.4. L√≥gica de Upsert (MERGE INTO) na Tabela Delta\n",
    "    # ------------------------------------------------------------------------------------------------------------------\n",
    "    try:\n",
    "        delta_table = DeltaTable.forPath(spark, BRONZE_TABLE_PATH)\n",
    "        \n",
    "        delta_table.alias(\"tgt\") \\\n",
    "            .merge(\n",
    "                df_transformed.alias(\"src\"),\n",
    "                \"tgt.PedidoID = src.PedidoID\"\n",
    "            ) \\\n",
    "            .whenMatchedUpdate(set={\n",
    "                \"EstabelecimentoID\": \"src.EstabelecimentoID\",\n",
    "                \"Produto\": \"src.Produto\",\n",
    "                \"quantidade_vendida\": \"src.quantidade_vendida\",\n",
    "                \"Preco_Unitario\": \"src.Preco_Unitario\",\n",
    "                \"data_venda\": \"src.data_venda\",\n",
    "                \"data_carga\": \"src.data_carga\"\n",
    "            }) \\\n",
    "            .whenNotMatchedInsert(values={\n",
    "                \"PedidoID\": \"src.PedidoID\",\n",
    "                \"EstabelecimentoID\": \"src.EstabelecimentoID\",\n",
    "                \"Produto\": \"src.Produto\",\n",
    "                \"quantidade_vendida\": \"src.quantidade_vendida\",\n",
    "                \"Preco_Unitario\": \"src.Preco_Unitario\",\n",
    "                \"data_venda\": \"src.data_venda\",\n",
    "                \"data_carga\": \"src.data_carga\"\n",
    "            }) \\\n",
    "            .execute()\n",
    "        \n",
    "    except (AnalysisException, ValueError) as e:\n",
    "        if \"is not a Delta table\" in str(e) or \"Path does not exist\" in str(e):\n",
    "            (\n",
    "                df_transformed.write.format(\"delta\")\n",
    "                    .partitionBy(\"data_carga\")\n",
    "                    .option(\"mergeSchema\", \"true\")\n",
    "                    .mode(\"append\")\n",
    "                    .save(BRONZE_TABLE_PATH)\n",
    "            )\n",
    "        else:\n",
    "            raise e\n",
    "    \n",
    "    # ------------------------------------------------------------------------------------------------------------------\n",
    "    # 2.5. Registro no Cat√°logo e Otimiza√ß√£o da Tabela Delta\n",
    "    # ------------------------------------------------------------------------------------------------------------------\n",
    "    spark.sql(f\"CREATE DATABASE IF NOT EXISTS {TABLE_NAME.split('.')[0]}\")\n",
    "    spark.sql(f\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS {TABLE_NAME}\n",
    "        USING DELTA\n",
    "        LOCATION '{BRONZE_TABLE_PATH}'\n",
    "    \"\"\")\n",
    "\n",
    "    spark.sql(f\"OPTIMIZE {TABLE_NAME} ZORDER BY (PedidoID)\")\n",
    "    spark.sql(f\"VACUUM {TABLE_NAME} RETAIN 168 HOURS\")\n",
    "\n",
    "    # Se o bloco `try` foi executado sem erros, o status √© de sucesso.\n",
    "    status = \"SUCESSO\"\n",
    "    erro = None\n",
    "\n",
    "except Exception as e:\n",
    "    # Se ocorrer qualquer erro, captura a mensagem de erro.\n",
    "    qtd_linhas = 0 # Define a quantidade de linhas como 0 em caso de falha.\n",
    "    erro = str(e)\n",
    "    print(f\"‚ö†Ô∏è Erro durante a execu√ß√£o do pipeline: {erro}\")\n",
    "\n",
    "finally:\n",
    "    # ------------------------------------------------------------------------------------------------------------------\n",
    "    # ‚úÖ 3. L√≥gica de Log - Executada sempre, independentemente de erro\n",
    "    # ------------------------------------------------------------------------------------------------------------------\n",
    "    # Calcula o tempo total de execu√ß√£o.\n",
    "    tempo_total = round(time.time() - start_time, 2)\n",
    "    \n",
    "    # Define o schema do log de forma expl√≠cita.\n",
    "    log_schema = StructType([\n",
    "        StructField(\"job_name\", StringType(), True),\n",
    "        StructField(\"data_execucao\", TimestampType(), True),\n",
    "        StructField(\"qtd_linhas\", IntegerType(), True),\n",
    "        StructField(\"status\", StringType(), True),\n",
    "        StructField(\"erro\", StringType(), True),\n",
    "        StructField(\"tempo_total_segundos\", DoubleType(), True)\n",
    "    ])\n",
    "\n",
    "    # Cria o DataFrame de log com os metadados da execu√ß√£o.\n",
    "    log_df = spark.createDataFrame([(\n",
    "        job_name,\n",
    "        None,\n",
    "        qtd_linhas,\n",
    "        status,\n",
    "        erro,\n",
    "        tempo_total\n",
    "    )], schema=log_schema).withColumn(\"data_execucao\", current_timestamp())\n",
    "\n",
    "    # Escreve o log na tabela Delta correspondente.\n",
    "    log_df.write.format(\"delta\") \\\n",
    "        .mode(\"append\") \\\n",
    "        .option(\"mergeSchema\", \"true\") \\\n",
    "        .save(LOG_PATH)\n",
    "\n",
    "    # Cria a tabela externa de log no cat√°logo.\n",
    "    spark.sql(f\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS {LOG_TABLE}\n",
    "        USING DELTA\n",
    "        LOCATION '{LOG_PATH}'\n",
    "    \"\"\")\n",
    "\n",
    "    # ------------------------------------------------------------------------------------------------------------------\n",
    "    # ‚úÖ 4. Confirma√ß√£o visual no notebook\n",
    "    # ------------------------------------------------------------------------------------------------------------------\n",
    "    print(\"‚úÖ Log de execu√ß√£o registrado com sucesso!\")\n",
    "    print(f\"üìå Job: {job_name}\")\n",
    "    print(f\"üì¶ Status: {status} | Registros: {qtd_linhas} | Dura√ß√£o: {tempo_total} segundos\")\n",
    "    if erro:\n",
    "        print(f\"‚ö†Ô∏è Detalhe do erro: {erro}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "71f07623-992f-4edf-b280-192c62ce4ba6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>job_name</th><th>data_execucao</th><th>qtd_linhas</th><th>status</th><th>erro</th><th>tempo_total_segundos</th><th>qtd_linhas_processadas</th></tr></thead><tbody><tr><td>bronze_pedidos</td><td>2025-05-28T02:25:42.507Z</td><td>null</td><td>ERRO</td><td>Falha na grava√ß√£o dos dados no Delta Lake (overwrite): [_LEGACY_ERROR_TEMP_DELTA_0007] A schema mismatch detected when writing to the Delta table (Table ID: 5dec149a-4a40-47b6-87ed-fb4a792dbbdc).\n",
       "To enable schema migration using DataFrameWriter or DataStreamWriter, please set:\n",
       "'.option(\"mergeSchema\", \"true\")'.\n",
       "For other operations, set the session configuration\n",
       "spark.databricks.delta.schema.autoMerge.enabled to \"true\". See the documentation\n",
       "specific to the operation for details.\n",
       "\n",
       "Table schema:\n",
       "root\n",
       "\n",
       "\n",
       "Data schema:\n",
       "root\n",
       "-- PedidoID: string (nullable = true)\n",
       "-- EstabelecimentoID: string (nullable = true)\n",
       "-- Produto: string (nullable = true)\n",
       "-- quantidade_vendida: integer (nullable = true)\n",
       "-- Preco_Unitario: double (nullable = true)\n",
       "-- data_venda: string (nullable = true)\n",
       "-- data_carga: timestamp (nullable = true)\n",
       "\n",
       "         \n",
       "Partition columns do not match the partition columns of the table.\n",
       "Given: [`data_carga`]\n",
       "Table: []\n",
       "         \n",
       "To overwrite your schema or change partitioning, please set:\n",
       "'.option(\"overwriteSchema\", \"true\")'.\n",
       "\n",
       "Note that the schema can't be overwritten when using\n",
       "'replaceWhere'.\n",
       "         </td><td>2.92</td><td>250</td></tr><tr><td>bronze_pedidos</td><td>2025-05-28T02:07:17.636Z</td><td>0</td><td>ERRO</td><td>[_LEGACY_ERROR_TEMP_DELTA_0007] A schema mismatch detected when writing to the Delta table (Table ID: 5dec149a-4a40-47b6-87ed-fb4a792dbbdc).\n",
       "To enable schema migration using DataFrameWriter or DataStreamWriter, please set:\n",
       "'.option(\"mergeSchema\", \"true\")'.\n",
       "For other operations, set the session configuration\n",
       "spark.databricks.delta.schema.autoMerge.enabled to \"true\". See the documentation\n",
       "specific to the operation for details.\n",
       "\n",
       "Table schema:\n",
       "root\n",
       "\n",
       "\n",
       "Data schema:\n",
       "root\n",
       "-- PedidoID: string (nullable = true)\n",
       "-- EstabelecimentoID: string (nullable = true)\n",
       "-- Produto: string (nullable = true)\n",
       "-- quantidade_vendida: integer (nullable = true)\n",
       "-- Preco_Unitario: double (nullable = true)\n",
       "-- data_venda: string (nullable = true)\n",
       "-- data_carga: timestamp (nullable = true)\n",
       "\n",
       "         \n",
       "Partition columns do not match the partition columns of the table.\n",
       "Given: [`data_carga`]\n",
       "Table: []\n",
       "         </td><td>2.04</td><td>null</td></tr><tr><td>bronze_pedidos</td><td>2025-05-28T02:22:46.139Z</td><td>null</td><td>ERRO</td><td>Falha na opera√ß√£o MERGE: Schema da tabela alvo Delta em dbfs:/FileStore/Ampev/tables/bronze/pedidos √© incompat√≠vel para MERGE. Coluna 'PedidoID' n√£o encontrada no schema alvo. Schema alvo atual: StructType([]). Considere dropar e recriar a tabela Delta se os dados existentes n√£o forem essenciais.</td><td>2.6</td><td>250</td></tr><tr><td>bronze_pedidos</td><td>2025-05-28T02:17:31.718Z</td><td>null</td><td>ERRO</td><td>Falha na grava√ß√£o dos dados (MERGE/APPEND): [DELTA_MERGE_UNRESOLVED_EXPRESSION] Cannot resolve tgt.PedidoID in search condition given columns src.PedidoID, src.EstabelecimentoID, src.Produto, src.quantidade_vendida, src.Preco_Unitario, src.data_venda, src.data_carga.; line 1 pos 0</td><td>5.11</td><td>250</td></tr><tr><td>bronze_pedidos</td><td>2025-05-28T02:19:40.115Z</td><td>null</td><td>ERRO</td><td>Falha na opera√ß√£o MERGE: [DELTA_MERGE_UNRESOLVED_EXPRESSION] Cannot resolve tgt.PedidoID in search condition given columns src.PedidoID, src.EstabelecimentoID, src.Produto, src.quantidade_vendida, src.Preco_Unitario, src.data_venda, src.data_carga.; line 1 pos 0</td><td>3.14</td><td>250</td></tr><tr><td>bronze_pedidos</td><td>2025-05-28T01:49:50.998Z</td><td>0</td><td>ERRO</td><td>[DELTA_MERGE_UNRESOLVED_EXPRESSION] Cannot resolve tgt.PedidoID in search condition given columns src.PedidoID, src.EstabelecimentoID, src.Produto, src.quantidade_vendida, src.Preco_Unitario, src.data_venda, src.data_carga.; line 1 pos 0</td><td>2.86</td><td>null</td></tr><tr><td>bronze_pedidos</td><td>2025-05-28T01:55:09.415Z</td><td>0</td><td>ERRO</td><td>[DELTA_MERGE_UNRESOLVED_EXPRESSION] Cannot resolve tgt.PedidoID in search condition given columns src.PedidoID, src.EstabelecimentoID, src.Produto, src.quantidade_vendida, src.Preco_Unitario, src.data_venda, src.data_carga.; line 1 pos 0</td><td>1.8</td><td>null</td></tr><tr><td>bronze_pedidos</td><td>2025-05-28T01:59:32.481Z</td><td>0</td><td>ERRO</td><td>[DELTA_MERGE_UNRESOLVED_EXPRESSION] Cannot resolve tgt.PedidoID in search condition given columns src.PedidoID, src.EstabelecimentoID, src.Produto, src.quantidade_vendida, src.Preco_Unitario, src.data_venda, src.data_carga.; line 1 pos 0</td><td>2.0</td><td>null</td></tr><tr><td>bronze_pedidos</td><td>2025-05-28T02:04:25.287Z</td><td>0</td><td>ERRO</td><td>[DELTA_MERGE_UNRESOLVED_EXPRESSION] Cannot resolve tgt.PedidoID in search condition given columns src.PedidoID, src.EstabelecimentoID, src.Produto, src.quantidade_vendida, src.Preco_Unitario, src.data_venda, src.data_carga.; line 1 pos 0</td><td>1.75</td><td>null</td></tr><tr><td>bronze_pedidos</td><td>2025-05-15T01:03:22.831Z</td><td>0</td><td>ERRO</td><td>[DELTA_MERGE_UNRESOLVED_EXPRESSION] Cannot resolve tgt.PedidoID in search condition given columns src.PedidoID, src.EstabelecimentoID, src.Produto, src.quantidade_vendida, src.Preco_Unitario, src.data_venda, src.data_carga.; line 1 pos 0</td><td>1.51</td><td>null</td></tr><tr><td>bronze_pedidos</td><td>2025-05-28T02:28:42.013Z</td><td>null</td><td>SUCESSO</td><td>null</td><td>53.44</td><td>250</td></tr><tr><td>bronze_pedidos</td><td>2025-05-28T02:35:32.742Z</td><td>null</td><td>SUCESSO</td><td>null</td><td>46.22</td><td>250</td></tr><tr><td>bronze_pedidos</td><td>2025-07-30T22:39:49.037Z</td><td>null</td><td>SUCESSO</td><td>null</td><td>55.88</td><td>250</td></tr><tr><td>bronze_pedidos</td><td>2025-08-01T01:02:41.371Z</td><td>null</td><td>SUCESSO</td><td>null</td><td>64.8</td><td>250</td></tr><tr><td>bronze_pedidos</td><td>2025-05-28T02:39:05.939Z</td><td>250</td><td>SUCESSO</td><td>null</td><td>47.34</td><td>null</td></tr><tr><td>bronze_pedidos</td><td>2025-08-03T21:55:01.58Z</td><td>250</td><td>SUCESSO</td><td>null</td><td>49.36</td><td>null</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "bronze_pedidos",
         "2025-05-28T02:25:42.507Z",
         null,
         "ERRO",
         "Falha na grava√ß√£o dos dados no Delta Lake (overwrite): [_LEGACY_ERROR_TEMP_DELTA_0007] A schema mismatch detected when writing to the Delta table (Table ID: 5dec149a-4a40-47b6-87ed-fb4a792dbbdc).\nTo enable schema migration using DataFrameWriter or DataStreamWriter, please set:\n'.option(\"mergeSchema\", \"true\")'.\nFor other operations, set the session configuration\nspark.databricks.delta.schema.autoMerge.enabled to \"true\". See the documentation\nspecific to the operation for details.\n\nTable schema:\nroot\n\n\nData schema:\nroot\n-- PedidoID: string (nullable = true)\n-- EstabelecimentoID: string (nullable = true)\n-- Produto: string (nullable = true)\n-- quantidade_vendida: integer (nullable = true)\n-- Preco_Unitario: double (nullable = true)\n-- data_venda: string (nullable = true)\n-- data_carga: timestamp (nullable = true)\n\n         \nPartition columns do not match the partition columns of the table.\nGiven: [`data_carga`]\nTable: []\n         \nTo overwrite your schema or change partitioning, please set:\n'.option(\"overwriteSchema\", \"true\")'.\n\nNote that the schema can't be overwritten when using\n'replaceWhere'.\n         ",
         2.92,
         250
        ],
        [
         "bronze_pedidos",
         "2025-05-28T02:07:17.636Z",
         0,
         "ERRO",
         "[_LEGACY_ERROR_TEMP_DELTA_0007] A schema mismatch detected when writing to the Delta table (Table ID: 5dec149a-4a40-47b6-87ed-fb4a792dbbdc).\nTo enable schema migration using DataFrameWriter or DataStreamWriter, please set:\n'.option(\"mergeSchema\", \"true\")'.\nFor other operations, set the session configuration\nspark.databricks.delta.schema.autoMerge.enabled to \"true\". See the documentation\nspecific to the operation for details.\n\nTable schema:\nroot\n\n\nData schema:\nroot\n-- PedidoID: string (nullable = true)\n-- EstabelecimentoID: string (nullable = true)\n-- Produto: string (nullable = true)\n-- quantidade_vendida: integer (nullable = true)\n-- Preco_Unitario: double (nullable = true)\n-- data_venda: string (nullable = true)\n-- data_carga: timestamp (nullable = true)\n\n         \nPartition columns do not match the partition columns of the table.\nGiven: [`data_carga`]\nTable: []\n         ",
         2.04,
         null
        ],
        [
         "bronze_pedidos",
         "2025-05-28T02:22:46.139Z",
         null,
         "ERRO",
         "Falha na opera√ß√£o MERGE: Schema da tabela alvo Delta em dbfs:/FileStore/Ampev/tables/bronze/pedidos √© incompat√≠vel para MERGE. Coluna 'PedidoID' n√£o encontrada no schema alvo. Schema alvo atual: StructType([]). Considere dropar e recriar a tabela Delta se os dados existentes n√£o forem essenciais.",
         2.6,
         250
        ],
        [
         "bronze_pedidos",
         "2025-05-28T02:17:31.718Z",
         null,
         "ERRO",
         "Falha na grava√ß√£o dos dados (MERGE/APPEND): [DELTA_MERGE_UNRESOLVED_EXPRESSION] Cannot resolve tgt.PedidoID in search condition given columns src.PedidoID, src.EstabelecimentoID, src.Produto, src.quantidade_vendida, src.Preco_Unitario, src.data_venda, src.data_carga.; line 1 pos 0",
         5.11,
         250
        ],
        [
         "bronze_pedidos",
         "2025-05-28T02:19:40.115Z",
         null,
         "ERRO",
         "Falha na opera√ß√£o MERGE: [DELTA_MERGE_UNRESOLVED_EXPRESSION] Cannot resolve tgt.PedidoID in search condition given columns src.PedidoID, src.EstabelecimentoID, src.Produto, src.quantidade_vendida, src.Preco_Unitario, src.data_venda, src.data_carga.; line 1 pos 0",
         3.14,
         250
        ],
        [
         "bronze_pedidos",
         "2025-05-28T01:49:50.998Z",
         0,
         "ERRO",
         "[DELTA_MERGE_UNRESOLVED_EXPRESSION] Cannot resolve tgt.PedidoID in search condition given columns src.PedidoID, src.EstabelecimentoID, src.Produto, src.quantidade_vendida, src.Preco_Unitario, src.data_venda, src.data_carga.; line 1 pos 0",
         2.86,
         null
        ],
        [
         "bronze_pedidos",
         "2025-05-28T01:55:09.415Z",
         0,
         "ERRO",
         "[DELTA_MERGE_UNRESOLVED_EXPRESSION] Cannot resolve tgt.PedidoID in search condition given columns src.PedidoID, src.EstabelecimentoID, src.Produto, src.quantidade_vendida, src.Preco_Unitario, src.data_venda, src.data_carga.; line 1 pos 0",
         1.8,
         null
        ],
        [
         "bronze_pedidos",
         "2025-05-28T01:59:32.481Z",
         0,
         "ERRO",
         "[DELTA_MERGE_UNRESOLVED_EXPRESSION] Cannot resolve tgt.PedidoID in search condition given columns src.PedidoID, src.EstabelecimentoID, src.Produto, src.quantidade_vendida, src.Preco_Unitario, src.data_venda, src.data_carga.; line 1 pos 0",
         2.0,
         null
        ],
        [
         "bronze_pedidos",
         "2025-05-28T02:04:25.287Z",
         0,
         "ERRO",
         "[DELTA_MERGE_UNRESOLVED_EXPRESSION] Cannot resolve tgt.PedidoID in search condition given columns src.PedidoID, src.EstabelecimentoID, src.Produto, src.quantidade_vendida, src.Preco_Unitario, src.data_venda, src.data_carga.; line 1 pos 0",
         1.75,
         null
        ],
        [
         "bronze_pedidos",
         "2025-05-15T01:03:22.831Z",
         0,
         "ERRO",
         "[DELTA_MERGE_UNRESOLVED_EXPRESSION] Cannot resolve tgt.PedidoID in search condition given columns src.PedidoID, src.EstabelecimentoID, src.Produto, src.quantidade_vendida, src.Preco_Unitario, src.data_venda, src.data_carga.; line 1 pos 0",
         1.51,
         null
        ],
        [
         "bronze_pedidos",
         "2025-05-28T02:28:42.013Z",
         null,
         "SUCESSO",
         null,
         53.44,
         250
        ],
        [
         "bronze_pedidos",
         "2025-05-28T02:35:32.742Z",
         null,
         "SUCESSO",
         null,
         46.22,
         250
        ],
        [
         "bronze_pedidos",
         "2025-07-30T22:39:49.037Z",
         null,
         "SUCESSO",
         null,
         55.88,
         250
        ],
        [
         "bronze_pedidos",
         "2025-08-01T01:02:41.371Z",
         null,
         "SUCESSO",
         null,
         64.8,
         250
        ],
        [
         "bronze_pedidos",
         "2025-05-28T02:39:05.939Z",
         250,
         "SUCESSO",
         null,
         47.34,
         null
        ],
        [
         "bronze_pedidos",
         "2025-08-03T21:55:01.58Z",
         250,
         "SUCESSO",
         null,
         49.36,
         null
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "createTempViewForImplicitDf": true,
        "dataframeName": "_sqldf",
        "executionCount": 5
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "job_name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "data_execucao",
         "type": "\"timestamp\""
        },
        {
         "metadata": "{}",
         "name": "qtd_linhas",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "status",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "erro",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "tempo_total_segundos",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "qtd_linhas_processadas",
         "type": "\"integer\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "select * from bronze.logs_pedidos"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 3663355946075330,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Bronze_Pedidos",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}