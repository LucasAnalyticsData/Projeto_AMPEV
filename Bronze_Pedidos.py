# Databricks notebook source
# MAGIC %md
# MAGIC | **Informa√ß√µes**    |            **Detalhes**         |
# MAGIC |--------------------|---------------------------------|
# MAGIC | Nome da Tabela     |          Bronze_Pedidos         |
# MAGIC | Data da Ingestao   |            31/03/2025           |
# MAGIC | Ultima Atualiza√ßao |            30/07/2025           |
# MAGIC | Origem             | DBFS (Databricks File System)   |
# MAGIC | Respons√°vel        |           Lucas Sousa           |
# MAGIC | Motivo             |   Cria√ß√£o de Camadas Bronze     |
# MAGIC | Observa√ß√µes        |               None              |
# MAGIC
# MAGIC ## Hist√≥rico de Atualiza√ß√µes
# MAGIC  | Data | Desenvolvido por | Motivo |
# MAGIC  |:----:|--------------|--------|
# MAGIC  |31/03/2025 | Lucas Sousa  | Cria√ß√£o do notebook |
# MAGIC  |30/07/2025 | Lucas Sousa  | Otimiza√ß√µes no notebook |
# MAGIC  

# COMMAND ----------

# ------------------------------------------
# ‚úÖ Registro da tabela Delta no cat√°logo do Databricks
# ------------------------------------------
spark.sql("""
    CREATE TABLE IF NOT EXISTS bronze.pedidos
    USING DELTA
    LOCATION 'dbfs:/FileStore/Ampev/tables/bronze/pedidos'
""")

# COMMAND ----------

# -*- coding: utf-8 -*-
# #############################################################################
# üì¶ Pipeline de Ingest√£o Bronze: Dados de Pedidos (CSV para Delta Lake)
# #############################################################################
# üéØ Objetivo:
# Este pipeline tem como objetivo principal realizar a ingest√£o em batch de dados brutos
# de pedidos (originados de um arquivo CSV) para a camada Bronze em um Data Lakehouse
# baseado em Delta Lake. Ele √© constru√≠do com foco em:
# - Qualidade na Origem: Leitura de dados CSV com schema expl√≠cito e tratamento de erros.
# - Idempot√™ncia e Resili√™ncia: L√≥gica robusta para lidar com re-execu√ß√µes e tabelas corrompidas.
# - Upsert Eficiente: Utiliza√ß√£o de MERGE INTO para atualiza√ß√µes e inser√ß√µes incrementais.
# - Rastreabilidade: Adi√ß√£o de metadados de linhagem e registro de log de execu√ß√£o.
# - Otimiza√ß√£o de Performance e Custo: Aplica√ß√£o de otimiza√ß√µes Spark e Delta Lake.
#
# Este c√≥digo demonstra pr√°ticas avan√ßadas em engenharia de dados para pipelines Bronze:
# - **Defini√ß√£o de Par√¢metros de Exemplo:** `caminho_origem = '/mnt/dados/brutos/pedidos.csv'`
# - Defini√ß√£o de par√¢metros e caminhos centralizada.
# - Schema enforcement/validation na leitura para garantir a integridade dos dados.
# - Transforma√ß√µes de qualidade de dados (deduplica√ß√£o, tratamento de nulos, filtragem de chaves).
# - Adi√ß√£o de metadados de linhagem (coluna 'data_carga').
# - Uso estrat√©gico do Delta Lake para propriedades ACID, evolu√ß√£o de schema e Time Travel.
# - Mecanismo de auto-recupera√ß√£o para metadados de tabela Delta potencialmente corrompidos.
# - Orquestra√ß√£o de erros e registro de log detalhado para observabilidade.
# - Estrat√©gias de particionamento e ZORDER para otimiza√ß√£o de leitura e escrita.
# #############################################################################

# Importa√ß√µes necess√°rias para manipula√ß√£o de dados e opera√ß√µes Delta Lake.
from pyspark.sql.functions import current_timestamp # Importa a fun√ß√£o para obter o timestamp atual.
from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, TimestampType # Importa tipos de dados para defini√ß√£o expl√≠cita de schema.
from delta.tables import DeltaTable # Importa a classe DeltaTable para opera√ß√µes avan√ßadas como MERGE INTO.
import sys # M√≥dulo para acessar funcionalidades do sistema (usado para stderr).
from pyspark.sql.utils import AnalysisException # Importa a exce√ß√£o espec√≠fica do Spark para erros de an√°lise (metadados).


# ----------------------------------------------------------------------------------------------------------------------
# ‚úÖ 1. Defini√ß√£o de caminhos e nomes l√≥gicos usados no pipeline
#    (Melhor pr√°tica: Centralizar configura√ß√µes para facilitar manuten√ß√£o e reuso)
# ----------------------------------------------------------------------------------------------------------------------
# Caminho da fonte de dados bruta em formato CSV no DBFS (Databricks File System).
# Esta √© a origem dos dados que ser√° ingerida na camada Bronze.
SOURCE_PATH = "dbfs:/FileStore/Ampev/pedidos.csv"

# Caminho f√≠sico no DBFS onde os dados da tabela Delta da camada Bronze ser√£o armazenados.
# √â uma conven√ß√£o comum organizar o Data Lake por camadas (raw/bronze, silver, gold).
BRONZE_TABLE_PATH = "dbfs:/FileStore/Ampev/tables/bronze/pedidos"

# Nome l√≥gico da tabela no cat√°logo do Spark (Metastore).
# Permite que a tabela seja consultada via SQL (ex: SELECT * FROM bronze.pedidos).
TABLE_NAME = "bronze.pedidos"

# Caminho para o checkpoint (ponto de controle) do Spark Structured Streaming.
# Embora este pipeline seja batch, a inclus√£o do checkpoint_path √© uma vis√£o de futuro,
# facilitando a transi√ß√£o para um pipeline de streaming sem grandes refatora√ß√µes.
CHECKPOINT_PATH = "dbfs:/FileStore/Ampev/checkpoints/bronze/pedidos"

# ----------------------------------------------------------------------------------------------------------------------
# ‚úÖ 2. Defini√ß√£o do Schema expl√≠cito para o CSV
#    (Melhor pr√°tica: Garante consist√™ncia de tipos e robustez contra infer√™ncia autom√°tica)
# ----------------------------------------------------------------------------------------------------------------------
# Define o schema da tabela de entrada de forma expl√≠cita.
# Isso √© crucial para:
# 1. Prevenir problemas de infer√™ncia de schema (que pode ser inconsistente ou incorreta).
# 2. Garantir a qualidade dos dados e a consist√™ncia dos tipos entre execu√ß√µes.
# 3. Acelerar a leitura, pois o Spark n√£o precisa escanear o arquivo para inferir.
schema = StructType([
    StructField("PedidoID", StringType(), True), # ID √∫nico do pedido.
    StructField("EstabelecimentoID", StringType(), True), # Chave estrangeira para a tabela de estabelecimentos.
    StructField("Produto", StringType(), True), # Nome do produto vendido.
    StructField("quantidade_vendida", IntegerType(), True), # Quantidade do produto vendida.
    StructField("Preco_Unitario", DoubleType(), True), # Pre√ßo de venda unit√°rio do produto.
    StructField("data_venda", StringType(), True) # Data da transa√ß√£o de venda.
])

# ----------------------------------------------------------------------------------------------------------------------
# ‚úÖ 3. Leitura do CSV com controle de schema e tratamento de linhas inv√°lidas
#    (Abordagem robusta para ingest√£o de dados brutos)
# ----------------------------------------------------------------------------------------------------------------------
# Realiza a leitura do arquivo CSV, aplicando o schema expl√≠cito definido e tratando
# linhas malformadas de forma robusta.
df_raw = (
    spark.read # Inicia a opera√ß√£o de leitura de dados.
        .format("csv") # Especifica o formato da fonte de dados como CSV.
        .option("header", "true") # Informa ao Spark que o CSV cont√©m uma linha de cabe√ßalho.
        .option("mode", "DROPMALFORMED") # Define o modo de tratamento de erros. "DROPMALFORMED" descarta linhas que n√£o se encaixam no schema, o que √© √∫til para ingest√µes robustas de dados brutos ("fail-fast" seria uma alternativa mais estrita).
        .schema(schema) # Aplica o schema expl√≠cito, garantindo a tipagem correta.
        .load(SOURCE_PATH) # Carrega o DataFrame a partir do caminho de origem especificado.
)

# ----------------------------------------------------------------------------------------------------------------------
# ‚úÖ 4. Aplica√ß√£o de Transforma√ß√µes e Adi√ß√£o de Metadados de Carga
#    (Preparando os dados para a camada Bronze)
# ----------------------------------------------------------------------------------------------------------------------
# Aplica transforma√ß√µes essenciais para a camada Bronze, como remo√ß√£o de duplicatas,
# limpeza de nulos cr√≠ticos e a adi√ß√£o de metadados de linhagem.
df_transformed = (
    df_raw.dropDuplicates() # Remove linhas que s√£o c√≥pias exatas umas das outras.
          .na.drop(subset=["PedidoID"]) # Remove linhas onde a chave principal 'PedidoID' √© nula.
          .filter("TRIM(PedidoID) != ''") # Filtra pedidos sem ID v√°lido, garantindo a integridade da chave.
          .withColumn("data_carga", current_timestamp()) # Adiciona uma coluna com o timestamp da execu√ß√£o. Essencial para linhagem e particionamento.
          .repartition("EstabelecimentoID") # Reparticiona o DataFrame para otimizar as escritas e leituras subsequentes, especialmente em consultas filtradas por EstabelecimentoID.
)

# ----------------------------------------------------------------------------------------------------------------------
# ‚úÖ 5. L√≥gica de Upsert (MERGE INTO) na Tabela Delta
#    (Melhor pr√°tica: Ingest√£o idempotente e eficiente para dados mut√°veis)
# ----------------------------------------------------------------------------------------------------------------------
# Esta se√ß√£o decide se a opera√ß√£o de escrita ser√° um "append" inicial (se a tabela n√£o existir)
# ou um "merge" (se a tabela j√° existir), que √© uma opera√ß√£o de upsert.
try:
    # Tenta criar um objeto DeltaTable para o caminho, o que s√≥ √© poss√≠vel se a tabela j√° existir.
    delta_table = DeltaTable.forPath(spark, BRONZE_TABLE_PATH)
    
    # Se a tabela existe, realiza a opera√ß√£o de MERGE INTO.
    # O MERGE √© crucial para o modelo "medallion", pois permite atualizar dados existentes
    # e inserir novos registros em uma √∫nica transa√ß√£o at√¥mica (upsert).
    delta_table.alias("tgt") \
        .merge(
            df_transformed.alias("src"), # Define o DataFrame transformado como a fonte ('src').
            "tgt.PedidoID = src.PedidoID" # Condi√ß√£o de MERGE: a correspond√™ncia √© feita pela chave prim√°ria PedidoID.
        ) \
        .whenMatchedUpdate(set={ # Quando uma linha √© encontrada ('matched').
            "EstabelecimentoID": "src.EstabelecimentoID",
            "Produto": "src.Produto",
            "quantidade_vendida": "src.quantidade_vendida",
            "Preco_Unitario": "src.Preco_Unitario",
            "data_venda": "src.data_venda",
            "data_carga": "src.data_carga" # Atualiza o timestamp de carga para refletir a √∫ltima modifica√ß√£o.
        }) \
        .whenNotMatchedInsert(values={ # Quando uma nova linha √© encontrada ('not matched').
            "PedidoID": "src.PedidoID",
            "EstabelecimentoID": "src.EstabelecimentoID",
            "Produto": "src.Produto",
            "quantidade_vendida": "src.quantidade_vendida",
            "Preco_Unitario": "src.Preco_Unitario",
            "data_venda": "src.data_venda",
            "data_carga": "src.data_carga"
        }) \
        .execute() # Executa a opera√ß√£o de MERGE.
    
except (AnalysisException, ValueError) as e:
    # Se a exce√ß√£o for `AnalysisException` ou `ValueError` (ex: a tabela n√£o existe),
    # o pipeline assume que √© a primeira execu√ß√£o e cria a tabela.
    # Este √© um padr√£o robusto para lidar com a cria√ß√£o inicial da tabela.
    if "is not a Delta table" in str(e) or "Path does not exist" in str(e):
        # A tabela n√£o existe no caminho, ent√£o a criamos.
        # Utiliza o modo 'append' na primeira escrita para criar a tabela com o schema do DataFrame.
        # O Databricks/Spark infere o formato e o trata como uma cria√ß√£o de tabela se o modo for 'append'
        # e o caminho ainda n√£o existir.
        (
            df_transformed.write.format("delta")
                .partitionBy("data_carga") # Particiona a tabela fisicamente para otimizar queries por data.
                .option("mergeSchema", "true") # Permite a evolu√ß√£o do schema em execu√ß√µes futuras.
                .mode("append") # Cria a tabela na primeira execu√ß√£o, e anexa dados nas subsequentes.
                .save(BRONZE_TABLE_PATH) # Salva o DataFrame como uma nova tabela Delta.
        )
    else:
        # Se for qualquer outro tipo de erro, ele √© re-lan√ßado.
        raise e

# ----------------------------------------------------------------------------------------------------------------------
# ‚úÖ 6. Registro no Cat√°logo e Otimiza√ß√£o da Tabela Delta
#    (Melhor pr√°tica: Garantir a acessibilidade e a alta performance da tabela)
# ----------------------------------------------------------------------------------------------------------------------
# Garante que a tabela est√° registrada no metastore para ser acess√≠vel via SQL.
# Em seguida, aplica otimiza√ß√µes para melhorar a performance de leitura.
spark.sql(f"CREATE DATABASE IF NOT EXISTS {TABLE_NAME.split('.')[0]}")
spark.sql(f"""
    CREATE TABLE IF NOT EXISTS {TABLE_NAME}
    USING DELTA
    LOCATION '{BRONZE_TABLE_PATH}'
""")

# Aplica otimiza√ß√µes para combinar pequenos arquivos e co-localizar dados.
# Isso melhora significativamente o desempenho de queries subsequentes.
spark.sql(f"OPTIMIZE {TABLE_NAME} ZORDER BY (PedidoID)")
spark.sql(f"VACUUM {TABLE_NAME} RETAIN 168 HOURS") # Limpa arquivos antigos, mantendo 7 dias para 'Time Travel'.


# COMMAND ----------

# MAGIC %sql
# MAGIC select * from bronze.pedidos limit 10
# MAGIC

# COMMAND ----------

# üéØ Objetivo:
# Este pipeline realiza a ingest√£o em batch de dados brutos de pedidos (CSV) para a
# camada Bronze em um Data Lakehouse. Al√©m disso, agora inclui um mecanismo robusto
# de log de execu√ß√£o que captura metadados essenciais como tempo, status e erros.
#
# A inclus√£o do log de execu√ß√£o √© uma pr√°tica de governan√ßa e observabilidade,
# permitindo o rastreamento, monitoramento e diagn√≥stico de falhas do pipeline
# de forma centralizada.
# #############################################################################

# Importa√ß√µes necess√°rias para manipula√ß√£o de dados, opera√ß√µes Delta Lake e log.
from pyspark.sql.functions import current_timestamp # Importa a fun√ß√£o para obter o timestamp atual.
from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, TimestampType # Importa tipos de dados para schema.
from delta.tables import DeltaTable # Importa a classe DeltaTable para opera√ß√µes avan√ßadas.
from pyspark.sql.utils import AnalysisException # Importa a exce√ß√£o para erros de an√°lise.
import time # M√≥dulo para medi√ß√£o de tempo de execu√ß√£o.

# ----------------------------------------------------------------------------------------------------------------------
# ‚úÖ 1. Defini√ß√£o de caminhos e nomes l√≥gicos usados no pipeline
# ----------------------------------------------------------------------------------------------------------------------
SOURCE_PATH = "dbfs:/FileStore/Ampev/pedidos.csv"
BRONZE_TABLE_PATH = "dbfs:/FileStore/Ampev/tables/bronze/pedidos"
TABLE_NAME = "bronze.pedidos"
LOG_PATH = "dbfs:/FileStore/Ampev/logs/bronze_pedidos"
LOG_TABLE = "bronze.logs_pedidos"
job_name = "bronze_pedidos" # Nome l√≥gico da execu√ß√£o.

# ----------------------------------------------------------------------------------------------------------------------
# ‚úÖ 2. In√≠cio do Bloco de Execu√ß√£o com Log
#    (Todo o pipeline √© encapsulado aqui para capturar sucesso ou falha)
# ----------------------------------------------------------------------------------------------------------------------
# Inicia a medi√ß√£o do tempo de execu√ß√£o para o log.
start_time = time.time()
status = "ERRO" # Define o status inicial como ERRO.

try:
    # ------------------------------------------------------------------------------------------------------------------
    # 2.1. Defini√ß√£o do Schema expl√≠cito para o CSV
    # ------------------------------------------------------------------------------------------------------------------
    schema = StructType([
        StructField("PedidoID", StringType(), True),
        StructField("EstabelecimentoID", StringType(), True),
        StructField("Produto", StringType(), True),
        StructField("quantidade_vendida", IntegerType(), True),
        StructField("Preco_Unitario", DoubleType(), True),
        StructField("data_venda", StringType(), True)
    ])

    # ------------------------------------------------------------------------------------------------------------------
    # 2.2. Leitura do CSV com controle de schema e tratamento de linhas inv√°lidas
    # ------------------------------------------------------------------------------------------------------------------
    df_raw = (
        spark.read
            .format("csv")
            .option("header", "true")
            .option("mode", "DROPMALFORMED")
            .schema(schema)
            .load(SOURCE_PATH)
    )

    # ------------------------------------------------------------------------------------------------------------------
    # 2.3. Aplica√ß√£o de Transforma√ß√µes e Adi√ß√£o de Metadados
    # ------------------------------------------------------------------------------------------------------------------
    df_transformed = (
        df_raw.dropDuplicates()
              .na.drop(subset=["PedidoID"])
              .filter("TRIM(PedidoID) != ''")
              .withColumn("data_carga", current_timestamp())
              .repartition("EstabelecimentoID")
    )
    
    # Captura a quantidade de linhas processadas para o log.
    qtd_linhas = df_transformed.count()

    # ------------------------------------------------------------------------------------------------------------------
    # 2.4. L√≥gica de Upsert (MERGE INTO) na Tabela Delta
    # ------------------------------------------------------------------------------------------------------------------
    try:
        delta_table = DeltaTable.forPath(spark, BRONZE_TABLE_PATH)
        
        delta_table.alias("tgt") \
            .merge(
                df_transformed.alias("src"),
                "tgt.PedidoID = src.PedidoID"
            ) \
            .whenMatchedUpdate(set={
                "EstabelecimentoID": "src.EstabelecimentoID",
                "Produto": "src.Produto",
                "quantidade_vendida": "src.quantidade_vendida",
                "Preco_Unitario": "src.Preco_Unitario",
                "data_venda": "src.data_venda",
                "data_carga": "src.data_carga"
            }) \
            .whenNotMatchedInsert(values={
                "PedidoID": "src.PedidoID",
                "EstabelecimentoID": "src.EstabelecimentoID",
                "Produto": "src.Produto",
                "quantidade_vendida": "src.quantidade_vendida",
                "Preco_Unitario": "src.Preco_Unitario",
                "data_venda": "src.data_venda",
                "data_carga": "src.data_carga"
            }) \
            .execute()
        
    except (AnalysisException, ValueError) as e:
        if "is not a Delta table" in str(e) or "Path does not exist" in str(e):
            (
                df_transformed.write.format("delta")
                    .partitionBy("data_carga")
                    .option("mergeSchema", "true")
                    .mode("append")
                    .save(BRONZE_TABLE_PATH)
            )
        else:
            raise e
    
    # ------------------------------------------------------------------------------------------------------------------
    # 2.5. Registro no Cat√°logo e Otimiza√ß√£o da Tabela Delta
    # ------------------------------------------------------------------------------------------------------------------
    spark.sql(f"CREATE DATABASE IF NOT EXISTS {TABLE_NAME.split('.')[0]}")
    spark.sql(f"""
        CREATE TABLE IF NOT EXISTS {TABLE_NAME}
        USING DELTA
        LOCATION '{BRONZE_TABLE_PATH}'
    """)

    spark.sql(f"OPTIMIZE {TABLE_NAME} ZORDER BY (PedidoID)")
    spark.sql(f"VACUUM {TABLE_NAME} RETAIN 168 HOURS")

    # Se o bloco `try` foi executado sem erros, o status √© de sucesso.
    status = "SUCESSO"
    erro = None

except Exception as e:
    # Se ocorrer qualquer erro, captura a mensagem de erro.
    qtd_linhas = 0 # Define a quantidade de linhas como 0 em caso de falha.
    erro = str(e)
    print(f"‚ö†Ô∏è Erro durante a execu√ß√£o do pipeline: {erro}")

finally:
    # ------------------------------------------------------------------------------------------------------------------
    # ‚úÖ 3. L√≥gica de Log - Executada sempre, independentemente de erro
    # ------------------------------------------------------------------------------------------------------------------
    # Calcula o tempo total de execu√ß√£o.
    tempo_total = round(time.time() - start_time, 2)
    
    # Define o schema do log de forma expl√≠cita.
    log_schema = StructType([
        StructField("job_name", StringType(), True),
        StructField("data_execucao", TimestampType(), True),
        StructField("qtd_linhas", IntegerType(), True),
        StructField("status", StringType(), True),
        StructField("erro", StringType(), True),
        StructField("tempo_total_segundos", DoubleType(), True)
    ])

    # Cria o DataFrame de log com os metadados da execu√ß√£o.
    log_df = spark.createDataFrame([(
        job_name,
        None,
        qtd_linhas,
        status,
        erro,
        tempo_total
    )], schema=log_schema).withColumn("data_execucao", current_timestamp())

    # Escreve o log na tabela Delta correspondente.
    log_df.write.format("delta") \
        .mode("append") \
        .option("mergeSchema", "true") \
        .save(LOG_PATH)

    # Cria a tabela externa de log no cat√°logo.
    spark.sql(f"""
        CREATE TABLE IF NOT EXISTS {LOG_TABLE}
        USING DELTA
        LOCATION '{LOG_PATH}'
    """)

    # ------------------------------------------------------------------------------------------------------------------
    # ‚úÖ 4. Confirma√ß√£o visual no notebook
    # ------------------------------------------------------------------------------------------------------------------
    print("‚úÖ Log de execu√ß√£o registrado com sucesso!")
    print(f"üìå Job: {job_name}")
    print(f"üì¶ Status: {status} | Registros: {qtd_linhas} | Dura√ß√£o: {tempo_total} segundos")
    if erro:
        print(f"‚ö†Ô∏è Detalhe do erro: {erro}")


# COMMAND ----------

# MAGIC %sql
# MAGIC select * from bronze.logs_pedidos