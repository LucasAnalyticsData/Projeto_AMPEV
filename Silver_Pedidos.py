# Databricks notebook source
# MAGIC %md
# MAGIC | **Informa√ß√µes**    |            **Detalhes**         |
# MAGIC |--------------------|---------------------------------|
# MAGIC | Nome da Tabela     |          Silver_Pedidos         |
# MAGIC | Data da Ingestao   |            31/03/2025           |
# MAGIC | Ultima Atualiza√ßao |            30/07/2025           |
# MAGIC | Origem             | bronze.pedidos/bronze.estabelecimentos   |
# MAGIC | Respons√°vel        |           Lucas Sousa           |
# MAGIC | Motivo             |   Cria√ß√£o de Camadas Silver     |
# MAGIC | Observa√ß√µes        |               None              |
# MAGIC
# MAGIC ## Hist√≥rico de Atualiza√ß√µes
# MAGIC  | Data | Desenvolvido por | Motivo |
# MAGIC  |:----:|--------------|--------|
# MAGIC  |27/05/2025 | Lucas Sousa  | Cria√ß√£o do notebook |
# MAGIC  |30/07/2025 | Lucas Sousa  | Otimiza√ß√µes no notebook |
# MAGIC

# COMMAND ----------

# -*- coding: utf-8 -*-
# #############################################################################
# üì¶ Pipeline Silver - Transforma√ß√£o e Enriquecimento de Pedidos
# Autor: Lucas Sousa, especialista em Engenharia de Dados
# #############################################################################
# üéØ Objetivo do C√≥digo:
# Este script tem a miss√£o de atuar como o principal orquestrador do pipeline
# da camada Silver. Ele executa as seguintes tarefas:
# 1. L√™ os dados brutos de 'pedidos' e 'estabelecimentos' da camada Bronze.
# 2. Aplica transforma√ß√µes de limpeza, tipagem correta e deduplica√ß√£o.
# 3. Enriquecimento: Une os dados de pedidos com as informa√ß√µes dos estabelecimentos.
# 4. Persist√™ncia: Grava o resultado na camada Silver usando Delta Lake,
#    garantindo idempot√™ncia (evita duplicatas na re-execu√ß√£o) e evolu√ß√£o de schema.
# 5. Governan√ßa: Registra a tabela no metastore do Spark com documenta√ß√£o (coment√°rios).
# 6. Otimiza√ß√£o: Compacta e indexa a tabela Delta para acelerar consultas futuras.
#
# Demonstra dom√≠nio t√©cnico em:
# - Modelagem de dados (Bronze -> Silver).
# - Uso de SQL e DataFrames para transforma√ß√µes.
# - Gerenciamento de tabela Delta Lake (MERGE, OPTIMIZE, VACUUM).
# - Tratamento de erros e l√≥gica robusta de escrita (cria√ß√£o vs. atualiza√ß√£o).
# - Boas pr√°ticas de governan√ßa (metadados e coment√°rios).
# - Automa√ß√£o de pipelines de dados.
# #############################################################################

# -----------------------------------------------------------------------------
# üß© M√≥dulo 1: Importa√ß√µes Essenciais
# -----------------------------------------------------------------------------
# Importa fun√ß√µes do Spark SQL para manipula√ß√£o de colunas e express√µes.
from pyspark.sql.functions import current_date, current_timestamp, expr 
# Importa a classe principal para interagir com tabelas Delta.
from delta.tables import DeltaTable
# Importa m√≥dulo para tratamento de exce√ß√µes espec√≠ficas do Spark.
from pyspark.sql.utils import AnalysisException
# Importa m√≥dulo para interagir com o sistema de arquivos do Databricks (DBFS).
import os 
import sys # M√≥dulo para escrever logs de erro no stderr.

# -----------------------------------------------------------------------------
# ‚öôÔ∏è M√≥dulo 2: Defini√ß√µes de Par√¢metros e Metadados
# -----------------------------------------------------------------------------
# Define o nome do banco de dados (schema) para a camada Silver.
database = "silver"
# Define o nome da tabela de destino.
tabela = "pedidos"
# Constr√≥i o nome completo da tabela para uso em comandos SQL.
tabela_completa = f"{database}.{tabela}"
# Define o caminho f√≠sico (localiza√ß√£o) da tabela Delta no DBFS.
silver_path = "dbfs:/FileStore/Ampev/silver/tables/pedidos"

# Coment√°rios de documenta√ß√£o para a tabela e suas colunas (governan√ßa de dados).
# Essa documenta√ß√£o √© crucial para que os usu√°rios finais (analistas, cientistas de dados)
# entendam a origem, o prop√≥sito e a estrutura dos dados.
comentario_tabela = "Esta tabela cont√©m pedidos enriquecidos com dados de estabelecimentos para an√°lise corporativa, pronta para consumo em relat√≥rios e dashboards."
comentarios_colunas = {
    'id_pedido': 'Identificador √∫nico do pedido, agora com tipagem INT.',
    'id_estabelecimento': 'C√≥digo do estabelecimento associado ao pedido. Mantido como STRING para compatibilidade com a origem e evitar erros de CAST.',
    'produto': 'Nome do produto vendido no pedido.',
    'quantidade': 'Quantidade de itens vendidos, tipado como INT.',
    'preco': 'Pre√ßo unit√°rio do produto, tipado como DECIMAL para precis√£o financeira.',
    'nome_estabelecimento': 'Nome comercial do estabelecimento, obtido atrav√©s do JOIN com a tabela de estabelecimentos.',
    'email': 'Email de contato do estabelecimento, para comunica√ß√£o e valida√ß√£o.',
    'telefone': 'Telefone principal do estabelecimento.',
    'data_pedido': 'Data da realiza√ß√£o do pedido, tipado como DATE.',
    'data_carga': 'Data da carga t√©cnica para a camada Silver, usada para particionamento e auditoria.',
    'data_hora_carga': 'Timestamp da carga exato (UTC-3) para granularidade de auditoria.'
}

# -----------------------------------------------------------------------------
# üõ†Ô∏è M√≥dulo 3: Fun√ß√µes Auxiliares de Governan√ßa
# -----------------------------------------------------------------------------
def adiciona_comentarios_tabela(full_table_name: str, table_comment: str, column_comments: dict):
    """
    Aplica coment√°rios descritivos a uma tabela e suas colunas no metastore do Spark.

    Args:
        full_table_name (str): Nome completo da tabela (ex: "silver.pedidos").
        table_comment (str): Coment√°rio para a tabela.
        column_comments (dict): Dicion√°rio com nomes das colunas e seus coment√°rios.
    """
    print(f"üìä [INFO] Aplicando coment√°rios na tabela '{full_table_name}' e suas colunas...")
    try:
        # Tenta aplicar o coment√°rio na tabela.
        spark.sql(f"COMMENT ON TABLE {full_table_name} IS '{table_comment}'")
        # Itera sobre o dicion√°rio de coment√°rios e aplica cada um nas colunas.
        for col_name, col_comment in column_comments.items():
            spark.sql(f"ALTER TABLE {full_table_name} CHANGE COLUMN {col_name} COMMENT '{col_comment}'")
        print(f"üü¢ [INFO] Coment√°rios aplicados com sucesso para '{full_table_name}'.")
    except AnalysisException as e:
        # Captura erros de cat√°logo e trata de forma n√£o fatal, pois o pipeline j√° salvou os dados.
        print(f"‚ö†Ô∏è [AVISO] Falha ao aplicar coment√°rios. Tabela '{full_table_name}' pode n√£o estar registrada no metastore. Erro: {e}", file=sys.stderr)
    except Exception as e:
        print(f"‚ö†Ô∏è [AVISO] Ocorreu um erro inesperado ao aplicar coment√°rios. Erro: {e}", file=sys.stderr)


# -----------------------------------------------------------------------------
# üöÄ M√≥dulo 4: L√≥gica Principal de Transforma√ß√£o e Escrita
# -----------------------------------------------------------------------------
try:
    print("‚ñ∂Ô∏è [STATUS] Iniciando o pipeline Silver de pedidos.")

    # 1. Passo Cr√≠tico de Governan√ßa: Garante que o banco de dados 'silver' exista.
    # Esta √© uma pr√°tica robusta que evita o erro `TABLE_OR_VIEW_NOT_FOUND`
    # caso o schema n√£o tenha sido criado manualmente ou por um job anterior.
    print(f"üü° [INFO] Verificando e criando o banco de dados '{database}' se necess√°rio.")
    spark.sql(f"CREATE DATABASE IF NOT EXISTS {database}")

    # 2. Leitura e Enriquecimento:
    # A query utiliza Common Table Expressions (CTEs) para melhorar a legibilidade e organizar
    # o pipeline de transforma√ß√µes, seguindo uma pr√°tica de especialista.
    # O JOIN Left √© usado para garantir que todos os pedidos da Bronze sejam mantidos,
    # mesmo que n√£o haja um estabelecimento correspondente (o que resultaria em nulos).
    df_pedidos_silver = spark.sql(f"""
        WITH pedidos_limpos AS (
            SELECT
                CAST(PedidoID AS INT) AS id_pedido,
                CAST(EstabelecimentoID AS STRING) AS id_estabelecimento,
                Produto AS produto,
                CAST(quantidade_vendida AS INT) AS quantidade,
                CAST(Preco_Unitario AS DECIMAL(20,2)) AS preco,
                CAST(data_venda AS DATE) AS data_pedido
            FROM bronze.pedidos
            WHERE PedidoID IS NOT NULL
        ),
        estabelecimentos_limpos AS (
            SELECT
                EstabelecimentoID AS id_estabelecimento,
                Local AS nome_estabelecimento,
                Email AS email,
                Telefone AS telefone
            FROM bronze.estabelecimentos
        )
        SELECT
            ped.id_pedido,
            ped.id_estabelecimento,
            ped.produto,
            ped.quantidade,
            ped.preco,
            est.nome_estabelecimento,
            est.email,
            est.telefone,
            ped.data_pedido
        FROM pedidos_limpos ped
        LEFT JOIN estabelecimentos_limpos est
        ON ped.id_estabelecimento = est.id_estabelecimento
    """)
    print("üü¢ [INFO] Leitura e enriquecimento de dados conclu√≠dos.")
    
    # 3. Adi√ß√£o de Colunas T√©cnicas de Auditoria:
    # Adiciona colunas para rastrear a data e hora da carga, crucial para linhagem e auditoria.
    df_pedidos_silver = df_pedidos_silver.withColumn("data_carga", current_date())
    # O fuso hor√°rio √© ajustado para evitar inconsist√™ncias.
    df_pedidos_silver = df_pedidos_silver.withColumn("data_hora_carga", expr("current_timestamp() - INTERVAL 3 HOURS"))
    print("üü¢ [INFO] Colunas de auditoria adicionadas.")
    
    # 4. Verifica√ß√£o e Escrita da Tabela Delta:
    # A l√≥gica foi aprimorada para garantir que, independentemente da carga,
    # as opera√ß√µes de grava√ß√£o e registro no cat√°logo estejam sincronizadas.
    
    # Verifica se a tabela Delta j√° existe fisicamente no DBFS.
    if DeltaTable.isDeltaTable(spark, silver_path):
        # Se a tabela existe, usamos MERGE INTO para garantir UPSERT (atualiza√ß√£o ou inser√ß√£o).
        # A carga incremental √© mais robusta com MERGE do que com APPEND + deduplica√ß√£o posterior.
        print("üü° [INFO] Tabela Delta j√° existe. Realizando opera√ß√£o MERGE para carga incremental.")
        delta_table = DeltaTable.forPath(spark, silver_path)
        
        # A l√≥gica MERGE √© a forma mais robusta e eficiente de lidar com cargas incrementais
        # e deduplica√ß√£o sem ter que reprocessar toda a tabela.
        delta_table.alias("tgt") \
            .merge(
                df_pedidos_silver.alias("src"),
                "tgt.id_pedido = src.id_pedido" # Condi√ß√£o de jun√ß√£o para a MERGE.
            ) \
            .whenMatchedUpdateAll() \
            .whenNotMatchedInsertAll() \
            .execute()
        
        print("üü¢ [INFO] MERGE INTO conclu√≠do com sucesso.")
        
    else:
        # Se a tabela n√£o existe, fazemos a primeira carga.
        print("üü° [INFO] Tabela Delta n√£o existe. Realizando a primeira carga e cria√ß√£o da tabela.")
        (
            df_pedidos_silver.write
            .format("delta")
            .mode("overwrite") # 'overwrite' na primeira carga para garantir um estado limpo.
            .option("mergeSchema", "true") # Permite a evolu√ß√£o de schema j√° na primeira carga.
            .partitionBy("data_carga") # Particionamento para otimizar leituras.
            .save(silver_path)
        )
        print("üü¢ [INFO] Dados salvos e tabela Delta criada no DBFS.")

    # 5. Governan√ßa e Otimiza√ß√£o:
    # Esta √© a principal altera√ß√£o para corrigir o erro. A cria√ß√£o da tabela no cat√°logo
    # agora √© chamada em ambos os cen√°rios (primeira carga ou carga incremental), garantindo que
    # o metadado no cat√°logo do Spark esteja sempre sincronizado com o caminho f√≠sico do Delta.
    # Isso resolve o erro `TABLE_OR_VIEW_NOT_FOUND` se a tabela existir fisicamente, mas n√£o
    # logicamente no metastore.
    print("üü¢ [INFO] Garantindo o registro da tabela no metastore.")
    spark.sql(f"""
        CREATE TABLE IF NOT EXISTS {tabela_completa}
        USING DELTA
        LOCATION '{silver_path}'
    """)
    print("üü¢ [INFO] Tabela registrada/sincronizada com sucesso.")
    
    print("üü¢ [INFO] Iniciando tarefas de governan√ßa e otimiza√ß√£o.")
    adiciona_comentarios_tabela(tabela_completa, comentario_tabela, comentarios_colunas)
    
    # Otimiza√ß√£o com Z-Ordering √© uma t√©cnica avan√ßada que co-localiza dados relacionados
    # em arquivos no DBFS, acelerando consultas que filtram por essas colunas.
    # O 'id_pedido' √© uma √≥tima escolha para Z-Ordering.
    spark.sql(f"OPTIMIZE {tabela_completa} ZORDER BY (id_pedido)")
    
    # VACUUM remove arquivos de dados que n√£o s√£o mais referenciados pela tabela Delta,
    # limpando o hist√≥rico e liberando espa√ßo em disco. O RETENTION garante a seguran√ßa.
    spark.sql(f"VACUUM {tabela_completa} RETAIN 168 HOURS") # 168 horas = 7 dias.
    
    print("üöÄ [STATUS] Pipeline Silver conclu√≠do com sucesso! Tabela otimizada e documentada.")

except Exception as e:
    # O bloco 'except' captura qualquer erro grave e exibe uma mensagem clara.
    print(f"‚ùå [ERRO CR√çTICO] O pipeline Silver falhou. Erro: {str(e)}", file=sys.stderr)
    raise e # Re-lan√ßa o erro para que a orquestra√ß√£o externa (como um job) possa captur√°-lo.


# COMMAND ----------

# MAGIC %sql
# MAGIC SELECT * FROM silver.pedidos LIMIT 20

# COMMAND ----------

# üìã Registro de Log de Execu√ß√£o Delta ‚Äì Camada Silver
# üéØ Objetivo: rastrear a execu√ß√£o da camada Silver de forma audit√°vel e resiliente

from pyspark.sql.functions import current_timestamp
from pyspark.sql.types import StructType, StructField, StringType, TimestampType, IntegerType, DoubleType
import time

# ------------------------------------------
# ‚úÖ 1. In√≠cio da medi√ß√£o de tempo da execu√ß√£o
# ------------------------------------------
start_time = time.time()

# ------------------------------------------
# ‚úÖ 2. Identifica√ß√£o l√≥gica do job atual
# ------------------------------------------
job_name = "silver_pedidos"  # üîÅ Altere conforme o pipeline

try:
    # ----------------------------------------------------
    # ‚úÖ 3. Simula√ß√£o de processamento da camada Silver
    #    Substitua este trecho pelo seu pipeline real
    # ----------------------------------------------------
    df = spark.table("silver.pedidos")  # Exemplo: leitura da tabela j√° processada
    qtd_linhas = df.count()  # Contagem ap√≥s transforma√ß√£o

    status = "SUCESSO"
    erro = None

except Exception as e:
    # üß® Captura qualquer erro ocorrido durante o pipeline
    qtd_linhas = 0
    status = "ERRO"
    erro = str(e)

# ------------------------------------------
# ‚úÖ 4. Medi√ß√£o do tempo total de execu√ß√£o
# ------------------------------------------
tempo_total = round(time.time() - start_time, 2)

# ------------------------------------------
# ‚úÖ 5. Defini√ß√£o do schema expl√≠cito para o log Delta
# ------------------------------------------
log_schema = StructType([
    StructField("job_name", StringType(), True),
    StructField("data_execucao", TimestampType(), True),
    StructField("qtd_linhas", IntegerType(), True),
    StructField("status", StringType(), True),
    StructField("erro", StringType(), True),
    StructField("tempo_total_segundos", DoubleType(), True)
])

# ------------------------------------------
# ‚úÖ 6. Cria√ß√£o do DataFrame de log com metadados t√©cnicos
# ------------------------------------------
log_df = spark.createDataFrame([(
    job_name,
    None,  # Timestamp preenchido com a fun√ß√£o atualizada abaixo
    qtd_linhas,
    status,
    erro,
    tempo_total
)], schema=log_schema).withColumn("data_execucao", current_timestamp())

# ------------------------------------------
# ‚úÖ 7. Caminhos de armazenamento do log na camada Silver
# ------------------------------------------
LOG_PATH = "dbfs:/FileStore/Ampev/logs/silver_pedidos"  # Delta Storage
LOG_TABLE = "silver.logs_pedidos"  # Cat√°logo Delta

# ------------------------------------------
# ‚úÖ 8. Escrita segura e resiliente no Delta Lake
# - Garante que o log seja incremental e tolerante a mudan√ßas de schema
# ------------------------------------------
log_df.write.format("delta") \
    .mode("append") \
    .option("mergeSchema", "true") \
    .save(LOG_PATH)

# Cria a tabela Delta no cat√°logo se n√£o existir
spark.sql(f"""
    CREATE TABLE IF NOT EXISTS {LOG_TABLE}
    USING DELTA
    LOCATION '{LOG_PATH}'
""")

# ------------------------------------------
# ‚úÖ 9. Feedback visual para o engenheiro
# ------------------------------------------
print("‚úÖ Log de execu√ß√£o da camada Silver registrado com sucesso!")
print(f"üìå Job: {job_name}")
print(f"üì¶ Status: {status} | Registros processados: {qtd_linhas} | Dura√ß√£o: {tempo_total} segundos")
if erro:
    print(f"‚ö†Ô∏è Detalhes do erro registrado no log: {erro}")


# COMMAND ----------

# MAGIC %sql
# MAGIC SELECT * FROM silver.logs_pedidos