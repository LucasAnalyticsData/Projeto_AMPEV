# ğŸ“¦ Pipeline Bronze - IngestÃ£o e GovernanÃ§a de Dados de Estabelecimentos (CSV â Delta Lake)

ğŸ‘¨â€ğŸ’» **Autor:** Lucas Sousa Santos Oliveira  
ğŸ¯ **Especialista em FinanÃ§as em transiÃ§Ã£o para Engenharia de Dados** | PÃ³s-graduaÃ§Ã£o em Big Data e Cloud Computing

---

## ğŸ¯ Objetivo do Projeto

Este notebook implementa um **pipeline de ingestÃ£o robusto, escalÃ¡vel e idempotente** para carregar dados brutos de **estabelecimentos** (CSV) na **Camada Bronze** de um Data Lakehouse utilizando **Delta Lake** no Databricks.

O foco Ã© garantir:

- âœ… **Qualidade na origem** com schema enforcement e tratamento de dados invÃ¡lidos
- ğŸ” **Upsert eficiente** com `MERGE INTO` para cargas incrementais
- ğŸ§  **Rastreabilidade total** com colunas de metadados
- âš™ï¸ **OtimizaÃ§Ã£o de performance** com particionamento, `OPTIMIZE ZORDER` e `VACUUM`
- ğŸ›¡ **GovernanÃ§a e confiabilidade** com registro no metastore

---

## ğŸ§± Arquitetura do Pipeline

```mermaid
flowchart TD
    A[ğŸ“„ CSV - Estabelecimentos] --> B[ğŸ“¥ Leitura com Schema Enforcement]
    B --> C[ğŸ§¹ Limpeza e ValidaÃ§Ã£o Inicial]
    C --> D[ğŸ§  Enriquecimento com data_ingestao]
    D --> E[ğŸ§­ Reparticionamento EstratÃ©gico]
    E --> F{Tabela Delta existe?}
    F -- Sim --> G[ğŸ” MERGE INTO para UPSERT]
    F -- NÃ£o --> H[ğŸ†• CriaÃ§Ã£o Inicial da Tabela Delta]
    G & H --> I[ğŸ›¡ Registro no Metastore]
    I --> J[âš™ï¸ OPTIMIZE ZORDER BY (EstabelecimentoID)]
    J --> K[ğŸ§¼ VACUUM 168 HOURS]
```

---

## ğŸ“Š TÃ©cnicas e Boas PrÃ¡ticas Aplicadas

| TÃ©cnica / PrÃ¡tica                       | Objetivo / BenefÃ­cio |
|-----------------------------------------|----------------------|
| **Schema Enforcement (StructType)**    | Garante tipagem correta e evita inferÃªncia inconsistente |
| **Tratamento de registros invÃ¡lidos**   | Remove dados malformados prevenindo falhas no MERGE |
| **DeduplicaÃ§Ã£o e filtro de nulos**      | Assegura integridade da chave primÃ¡ria |
| **Coluna `data_ingestao`**              | Rastreabilidade e suporte a Time Travel |
| **Reparticionamento por chave**         | Melhora performance em escrita e consultas futuras |
| **`CREATE TABLE IF NOT EXISTS`**        | Garante visibilidade no catÃ¡logo e previne erros |
| **`MERGE INTO` (Upsert)**               | Atualiza e insere registros de forma incremental e ACID |
| **`OPTIMIZE ZORDER`**                   | Melhora filtragem e leitura |
| **`VACUUM`**                            | Reduz custo de armazenamento removendo arquivos obsoletos |

---

## ğŸ›  Fluxo Detalhado

### 1ï¸âƒ£ Leitura do CSV com Schema Enforcement

```python
from pyspark.sql.types import StructType, StructField, StringType

schema = StructType([
    StructField("Local", StringType(), True),
    StructField("Email", StringType(), True),
    StructField("EstabelecimentoID", StringType(), True),
    StructField("Telefone", StringType(), True)
])

df_raw = (
    spark.read.format("csv")
    .option("header", "true")
    .option("mode", "DROPMALFORMED")
    .schema(schema)
    .load("dbfs:/FileStore/Ampev/estabelecimentos.csv")
)
```

ğŸ“Œ *Por que?* â€” Evita inferÃªncia automÃ¡tica e garante consistÃªncia entre execuÃ§Ãµes.

---

### 2ï¸âƒ£ Limpeza e ValidaÃ§Ã£o

```python
df_clean = (
    df_raw.dropDuplicates()
          .na.drop()
          .filter("EstabelecimentoID IS NOT NULL AND TRIM(EstabelecimentoID) != ''")
)
```

ğŸ“Œ *Por que?* â€” Remove duplicatas, nulos e registros sem chave para evitar falhas no upsert.

---

### 3ï¸âƒ£ Enriquecimento e Reparticionamento

```python
from pyspark.sql.functions import current_timestamp

df_partitioned = (
    df_clean.withColumn("data_ingestao", current_timestamp())
            .repartition("EstabelecimentoID")
)
```

ğŸ“Œ *Por que?* â€” Adiciona rastreabilidade e melhora performance de escrita.

---

### 4ï¸âƒ£ Escrita com MERGE INTO

```python
from delta.tables import DeltaTable

if DeltaTable.isDeltaTable(spark, "dbfs:/FileStore/Ampev/tables/bronze/estabelecimentos"):
    delta_table = DeltaTable.forPath(spark, "dbfs:/FileStore/Ampev/tables/bronze/estabelecimentos")
    (
        delta_table.alias("tgt")
        .merge(df_partitioned.alias("src"), "tgt.EstabelecimentoID = src.EstabelecimentoID")
        .whenMatchedUpdateAll()
        .whenNotMatchedInsertAll()
        .execute()
    )
else:
    df_partitioned.write.format("delta")         .partitionBy("data_ingestao")         .option("mergeSchema", "true")         .mode("overwrite")         .save("dbfs:/FileStore/Ampev/tables/bronze/estabelecimentos")
```

ğŸ“Œ *Por que?* â€” Garante ingestÃ£o incremental sem sobrescrever dados Ã­ntegros.

---

### 5ï¸âƒ£ OtimizaÃ§Ã£o e ManutenÃ§Ã£o

```sql
OPTIMIZE bronze.estabelecimentos ZORDER BY (EstabelecimentoID);
VACUUM bronze.estabelecimentos RETAIN 168 HOURS;
```

ğŸ“Œ *Por que?* â€” Compacta arquivos, melhora filtragem e libera espaÃ§o.

---

## âœ… Resultado Esperado

| MÃ©trica                     | Valor                          |
|-----------------------------|--------------------------------|
| ğŸ“Œ Tabela Delta Criada       | `bronze.estabelecimentos`      |
| ğŸ”‘ Chave de NegÃ³cio         | `EstabelecimentoID`            |
| ğŸ§¾ Particionamento FÃ­sico   | `data_ingestao`                 |
| âš™ï¸ OtimizaÃ§Ãµes Aplicadas     | `OPTIMIZE ZORDER + VACUUM`     |
| ğŸ” Tipo de IngestÃ£o         | `Batch` + `MERGE INTO`         |

---

## ğŸ§  ConclusÃ£o

Este pipeline Ã© **produÃ§Ã£o-ready** e incorpora princÃ­pios modernos de engenharia de dados:

- ğŸ“Š Qualidade desde a origem
- ğŸ” IdempotÃªncia e resiliÃªncia
- âš¡ Performance otimizada para custo e velocidade
- ğŸ›¡ GovernanÃ§a e rastreabilidade completas
- ğŸš€ Pronto para evoluÃ§Ã£o futura com streaming, camada Silver e Gold
