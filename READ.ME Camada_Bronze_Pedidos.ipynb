{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e4210347-0638-4c28-9e7a-ed94408d0768",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# üì¶ Pipeline Bronze - Ingest√£o e Governan√ßa de Dados de Pedidos (CSV ‚ûù Delta Lake)\n",
    "\n",
    "üë®‚Äçüíª **Autor:** Lucas Sousa Santos Oliveira  \n",
    "üéØ **Especialista em Finan√ßas em transi√ß√£o para Engenharia de Dados** | P√≥s-gradua√ß√£o em Big Data e Cloud Computing\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Objetivo do Projeto\n",
    "\n",
    "Este notebook implementa um **pipeline de ingest√£o robusto e escal√°vel** para carregar dados brutos de **pedidos** (CSV) na **Camada Bronze** de um Data Lakehouse utilizando **Delta Lake** no Databricks.\n",
    "\n",
    "O pipeline garante:\n",
    "\n",
    "- ‚úÖ **Qualidade na origem** com schema enforcement\n",
    "- üîÅ **Upsert incremental** usando `MERGE INTO`\n",
    "- üß† **Rastreabilidade completa** com metadados t√©cnicos\n",
    "- ‚öôÔ∏è **Otimiza√ß√£o de performance** com particionamento, `OPTIMIZE ZORDER` e `VACUUM`\n",
    "- üõ° **Governan√ßa e confiabilidade** com registro no metastore\n",
    "\n",
    "---\n",
    "\n",
    "## üß± Arquitetura do Pipeline\n",
    "\n",
    "```mermaid\n",
    "flowchart TD\n",
    "    A[üìÑ CSV - Pedidos] --> B[üì• Leitura com Schema Enforcement]\n",
    "    B --> C[üßπ Limpeza e Valida√ß√£o Inicial]\n",
    "    C --> D[üß† Enriquecimento com data_ingestao]\n",
    "    D --> E[üß≠ Reparticionamento Estrat√©gico]\n",
    "    E --> F{Tabela Delta existe?}\n",
    "    F -- Sim --> G[üîÅ MERGE INTO para UPSERT]\n",
    "    F -- N√£o --> H[üÜï Cria√ß√£o Inicial da Tabela Delta]\n",
    "    G & H --> I[üõ° Registro no Metastore]\n",
    "    I --> J[‚öôÔ∏è OPTIMIZE ZORDER BY (PedidoID)]\n",
    "    J --> K[üßº VACUUM 168 HOURS]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üìä T√©cnicas e Boas Pr√°ticas Aplicadas\n",
    "\n",
    "| T√©cnica / Pr√°tica                       | Objetivo / Benef√≠cio |\n",
    "|-----------------------------------------|----------------------|\n",
    "| **Schema Enforcement (StructType)**    | Garante tipagem correta e evita infer√™ncia inconsistente |\n",
    "| **Tratamento de registros inv√°lidos**   | Remove dados malformados prevenindo falhas no MERGE |\n",
    "| **Deduplica√ß√£o e filtro de nulos**      | Assegura integridade da chave prim√°ria |\n",
    "| **Coluna `data_ingestao`**              | Rastreabilidade e suporte a Time Travel |\n",
    "| **Reparticionamento por chave**         | Melhora performance em escrita e consultas futuras |\n",
    "| **`CREATE TABLE IF NOT EXISTS`**        | Garante visibilidade no cat√°logo e previne erros |\n",
    "| **`MERGE INTO` (Upsert)**               | Atualiza e insere registros de forma incremental e ACID |\n",
    "| **`OPTIMIZE ZORDER`**                   | Melhora filtragem e leitura |\n",
    "| **`VACUUM`**                            | Reduz custo de armazenamento removendo arquivos obsoletos |\n",
    "\n",
    "---\n",
    "\n",
    "## üõ† Fluxo Detalhado\n",
    "\n",
    "### 1Ô∏è‚É£ Leitura do CSV com Schema Enforcement\n",
    "\n",
    "```python\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, DateType\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"PedidoID\", StringType(), True),\n",
    "    StructField(\"EstabelecimentoID\", StringType(), True),\n",
    "    StructField(\"Produto\", StringType(), True),\n",
    "    StructField(\"quantidade_vendida\", IntegerType(), True),\n",
    "    StructField(\"Preco_Unitario\", DoubleType(), True),\n",
    "    StructField(\"data_venda\", DateType(), True)\n",
    "])\n",
    "\n",
    "df_raw = (\n",
    "    spark.read.format(\"csv\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"mode\", \"DROPMALFORMED\")\n",
    "    .schema(schema)\n",
    "    .load(\"dbfs:/FileStore/Ampev/pedidos.csv\")\n",
    ")\n",
    "```\n",
    "\n",
    "üìå *Por que?* ‚Äî Evita infer√™ncia autom√°tica e garante consist√™ncia entre execu√ß√µes.\n",
    "\n",
    "---\n",
    "\n",
    "### 2Ô∏è‚É£ Limpeza e Valida√ß√£o\n",
    "\n",
    "```python\n",
    "df_clean = (\n",
    "    df_raw.dropDuplicates()\n",
    "          .na.drop()\n",
    "          .filter(\"PedidoID IS NOT NULL AND TRIM(PedidoID) != ''\")\n",
    ")\n",
    "```\n",
    "\n",
    "üìå *Por que?* ‚Äî Remove duplicatas, nulos e registros sem chave para evitar falhas no upsert.\n",
    "\n",
    "---\n",
    "\n",
    "### 3Ô∏è‚É£ Enriquecimento e Reparticionamento\n",
    "\n",
    "```python\n",
    "from pyspark.sql.functions import current_timestamp\n",
    "\n",
    "df_partitioned = (\n",
    "    df_clean.withColumn(\"data_ingestao\", current_timestamp())\n",
    "            .repartition(\"PedidoID\")\n",
    ")\n",
    "```\n",
    "\n",
    "üìå *Por que?* ‚Äî Adiciona rastreabilidade e melhora performance de escrita.\n",
    "\n",
    "---\n",
    "\n",
    "### 4Ô∏è‚É£ Escrita com MERGE INTO\n",
    "\n",
    "```python\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "if DeltaTable.isDeltaTable(spark, \"dbfs:/FileStore/Ampev/tables/bronze/pedidos\"):\n",
    "    delta_table = DeltaTable.forPath(spark, \"dbfs:/FileStore/Ampev/tables/bronze/pedidos\")\n",
    "    (\n",
    "        delta_table.alias(\"tgt\")\n",
    "        .merge(df_partitioned.alias(\"src\"), \"tgt.PedidoID = src.PedidoID\")\n",
    "        .whenMatchedUpdateAll()\n",
    "        .whenNotMatchedInsertAll()\n",
    "        .execute()\n",
    "    )\n",
    "else:\n",
    "    df_partitioned.write.format(\"delta\")         .partitionBy(\"data_ingestao\")         .option(\"mergeSchema\", \"true\")         .mode(\"overwrite\")         .save(\"dbfs:/FileStore/Ampev/tables/bronze/pedidos\")\n",
    "```\n",
    "\n",
    "üìå *Por que?* ‚Äî Garante ingest√£o incremental sem sobrescrever dados √≠ntegros.\n",
    "\n",
    "---\n",
    "\n",
    "### 5Ô∏è‚É£ Otimiza√ß√£o e Manuten√ß√£o\n",
    "\n",
    "```sql\n",
    "OPTIMIZE bronze.pedidos ZORDER BY (PedidoID);\n",
    "VACUUM bronze.pedidos RETAIN 168 HOURS;\n",
    "```\n",
    "\n",
    "üìå *Por que?* ‚Äî Compacta arquivos, melhora filtragem e libera espa√ßo.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Resultado Esperado\n",
    "\n",
    "| M√©trica                     | Valor                          |\n",
    "|-----------------------------|--------------------------------|\n",
    "| üìå Tabela Delta Criada       | `bronze.pedidos`               |\n",
    "| üîë Chave de Neg√≥cio         | `PedidoID`                     |\n",
    "| üßæ Particionamento F√≠sico   | `data_ingestao`                 |\n",
    "| ‚öôÔ∏è Otimiza√ß√µes Aplicadas     | `OPTIMIZE ZORDER + VACUUM`     |\n",
    "| üîÅ Tipo de Ingest√£o         | `Batch` + `MERGE INTO`         |\n",
    "\n",
    "---\n",
    "\n",
    "## üß† Conclus√£o\n",
    "\n",
    "Este pipeline √© **produ√ß√£o-ready** e incorpora princ√≠pios modernos de engenharia de dados:\n",
    "\n",
    "- üìä Qualidade desde a origem\n",
    "- üîÅ Idempot√™ncia e resili√™ncia\n",
    "- ‚ö° Performance otimizada para custo e velocidade\n",
    "- üõ° Governan√ßa e rastreabilidade completas\n",
    "- üöÄ Pronto para evolu√ß√£o futura com streaming, camada Silver e Gold"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "READ.ME Camada_Bronze_Pedidos",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}