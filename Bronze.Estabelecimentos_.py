# Databricks notebook source
# MAGIC %md
# MAGIC | **Informa√ß√µes**    |            **Detalhes**         |
# MAGIC |--------------------|---------------------------------|
# MAGIC | Nome da Tabela     |     Bronze_Estabelecimentos     |
# MAGIC | Data da Ingestao   |            31/03/2025           |
# MAGIC | Ultima Atualiza√ßao |            30/07/2025           |
# MAGIC | Origem             | DBFS (Databricks File System)   |
# MAGIC | Respons√°vel        |           Lucas Sousa           |
# MAGIC | Motivo             |   Cria√ß√£o de Camadas Bronze     |
# MAGIC | Observa√ß√µes        |               None              |
# MAGIC
# MAGIC ## Hist√≥rico de Atualiza√ß√µes
# MAGIC  | Data | Desenvolvido por | Motivo |
# MAGIC  |:----:|--------------|--------|
# MAGIC  |31/03/2025 | Lucas Sousa  | Cria√ß√£o do notebook |
# MAGIC  |30/07/2025 | Lucas Sousa  | Otimiza√ß√µes no notebook |

# COMMAND ----------

# MAGIC %sql
# MAGIC CREATE DATABASE IF NOT EXISTS spark_catalog.bronze
# MAGIC LOCATION 'dbfs:/FileStore/Ampev/tables/bronze/estabelecimentos';

# COMMAND ----------

# -*- coding: utf-8 -*-
# #############################################################################
# üì¶ Pipeline de Ingest√£o Bronze: Dados de Estabelecimentos (CSV para Delta Lake)
# #############################################################################
# üéØ Objetivo:
# Este pipeline tem como objetivo principal realizar a ingest√£o em batch de dados brutos
# de estabelecimentos (originados de um arquivo CSV) para a camada Bronze em um Data Lakehouse
# baseado em Delta Lake. Ele √© constru√≠do com foco em:
# - **Qualidade na Origem:** Leitura de dados CSV com schema expl√≠cito e tratamento de erros.
# - **Idempot√™ncia e Resili√™ncia:** L√≥gica robusta para lidar com re-execu√ß√µes e tabelas corrompidas.
# - **Upsert Eficiente:** Utiliza√ß√£o de MERGE INTO para atualiza√ß√µes e inser√ß√µes incrementais.
# - **Rastreabilidade:** Adi√ß√£o de metadados de linhagem e registro de log de execu√ß√£o.
# - **Otimiza√ß√£o de Performance e Custo:** Aplica√ß√£o de otimiza√ß√µes Spark e Delta Lake.
#
# Este c√≥digo demonstra pr√°ticas avan√ßadas em engenharia de dados para pipelines Bronze:
# - Defini√ß√£o de par√¢metros e caminhos centralizada.
# - Schema enforcement/validation na leitura para garantir a integridade dos dados.
# - Transforma√ß√µes de qualidade de dados (deduplica√ß√£o, tratamento de nulos, filtragem de chaves).
# - Adi√ß√£o de metadados de linhagem (coluna 'data_carga').
# - Uso estrat√©gico do Delta Lake para propriedades ACID, evolu√ß√£o de schema e Time Travel.
# - Mecanismo de auto-recupera√ß√£o para metadados de tabela Delta potencialmente corrompidos.
# - Orquestra√ß√£o de erros e registro de log detalhado para observabilidade.
# - Estrat√©gias de particionamento e ZORDER para otimiza√ß√£o de leitura e escrita.
# #############################################################################

# üì¶ Pipeline Bronze - Ingest√£o Batch de Estabelecimentos (CSV ‚ûù Delta)
# üß† Objetivo: Carregar dados brutos para a camada Bronze com qualidade, rastreabilidade,
#             upsert eficiente e otimiza√ß√µes do Delta Lake.
#             Este pipeline foi projetado para ser idempotente e resiliente.

# Importa√ß√µes necess√°rias para manipula√ß√£o de dados e opera√ß√µes Delta Lake.
from pyspark.sql.functions import current_timestamp # Importa a fun√ß√£o para obter o timestamp atual.
from pyspark.sql.types import StructType, StructField, StringType # Importa tipos de dados para defini√ß√£o expl√≠cita de schema.
from delta.tables import DeltaTable # Importa a classe DeltaTable para opera√ß√µes avan√ßadas como MERGE INTO.

# ----------------------------------------------------------------------------------------------------------------------
# ‚úÖ 1. Defini√ß√£o de caminhos e nomes l√≥gicos usados no pipeline
#    (Melhor pr√°tica: Centralizar configura√ß√µes para facilitar manuten√ß√£o e reuso)
# ----------------------------------------------------------------------------------------------------------------------
# Caminho da fonte de dados bruta em formato CSV no DBFS (Databricks File System).
# Esta √© a origem dos dados que ser√° ingerida na camada Bronze.
SOURCE_PATH = "dbfs:/FileStore/Ampev/estabelecimentos.csv"

# Caminho f√≠sico no DBFS onde os dados da tabela Delta da camada Bronze ser√£o armazenados.
# √â uma conven√ß√£o comum organizar o Data Lake por camadas (raw/bronze, silver, gold).
BRONZE_TABLE_PATH = "dbfs:/FileStore/Ampev/tables/bronze/estabelecimentos"

# Nome l√≥gico da tabela no cat√°logo do Spark (Metastore).
# Permite que a tabela seja consultada via SQL (ex: SELECT * FROM bronze.estabelecimentos).
TABLE_NAME = "bronze.estabelecimentos"

# Caminho para o checkpoint (ponto de controle) do Spark Structured Streaming.
# Embora este pipeline seja batch, a inclus√£o do checkpoint_path √© uma vis√£o de futuro,
# facilitando a transi√ß√£o para um pipeline de streaming sem grandes refatora√ß√µes.
CHECKPOINT_PATH = "dbfs:/FileStore/Ampev/checkpoints/bronze/estabelecimentos"

# ----------------------------------------------------------------------------------------------------------------------
# ‚úÖ 2. Defini√ß√£o do Schema expl√≠cito para o CSV
#    (Melhor pr√°tica: Garante consist√™ncia de tipos e robustez contra infer√™ncia autom√°tica)
# ----------------------------------------------------------------------------------------------------------------------
# Define o schema da tabela de entrada de forma expl√≠cita.
# Isso √© crucial para:
# 1. Prevenir problemas de infer√™ncia de schema (que pode ser inconsistente ou incorreta).
# 2. Garantir a qualidade dos dados e a consist√™ncia dos tipos entre execu√ß√µes.
# 3. Acelerar a leitura, pois o Spark n√£o precisa escanear o arquivo para inferir.
schema = StructType([
    StructField("Local", StringType(), True), # Campo 'Local', tipo String, permitindo nulos.
    StructField("Email", StringType(), True), # Campo 'Email', tipo String, permitindo nulos.
    StructField("EstabelecimentoID", StringType(), True), # Campo 'EstabelecimentoID', tipo String, permitindo nulos.
    StructField("Telefone", StringType(), True) # Campo 'Telefone', tipo String, permitindo nulos.
])

# ----------------------------------------------------------------------------------------------------------------------
# ‚úÖ 3. Leitura do CSV com controle de schema e tratamento de linhas inv√°lidas
#    (Abordagem robusta para ingest√£o de dados brutos)
# ----------------------------------------------------------------------------------------------------------------------
df_raw = (
    spark.read # Inicia a opera√ß√£o de leitura de dados.
        .format("csv") # Especifica o formato da fonte de dados como CSV.
        .option("header", "true") # Informa ao Spark que o CSV cont√©m uma linha de cabe√ßalho.
        .option("mode", "DROPMALFORMED") # Define o modo de tratamento de registros malformados.
                                         # - "DROPMALFORMED": Ignora (descarta) linhas que n√£o se encaixam no schema.
                                         # - Alternativas: "PERMISSIVE" (default, insere _corrupt_record) ou "FAILFAST" (aborta a opera√ß√£o).
                                         #   "DROPMALFORMED" √© comum na Bronze para ingest√£o r√°pida, mas "PERMISSIVE"
                                         #   pode ser prefer√≠vel para capturar e analisar dados ruins posteriormente.
        .schema(schema) # Aplica o schema explicitamente definido, garantindo a tipagem correta.
        .load(SOURCE_PATH) # Carrega os dados do caminho da fonte especificado.
)

# ----------------------------------------------------------------------------------------------------------------------
# ‚úÖ 4. Limpeza e valida√ß√£o inicial dos dados brutos
#    (Garante a integridade m√≠nima para a chave prim√°ria na camada Bronze)
# ----------------------------------------------------------------------------------------------------------------------
df_clean = (
    df_raw
        .dropDuplicates() # Remove linhas duplicadas em *todas* as colunas do DataFrame.
                          # Essencial para garantir a idempot√™ncia do pipeline e evitar dados redundantes.
        .na.drop() # Remove linhas que cont√™m pelo menos um valor nulo em *qualquer* coluna (comportamento padr√£o 'any').
                   # Para a Bronze, pode ser √∫til ser mais permissivo e tratar nulos na Silver,
                   # mas aqui removemos registros incompletos que podem prejudicar o MERGE.
        .filter("EstabelecimentoID IS NOT NULL AND TRIM(EstabelecimentoID) != ''") # Filtra para garantir que a chave
                                                                                # 'EstabelecimentoID' n√£o seja nula ou vazia.
                                                                                # Uma chave prim√°ria v√°lida √© crucial para o UPSERT.
)

# ----------------------------------------------------------------------------------------------------------------------
# ‚úÖ 5. Enriquecimento com coluna de auditoria
#    (Fundamental para rastreabilidade, governan√ßa de dados e Time Travel)
# ----------------------------------------------------------------------------------------------------------------------
# Adiciona uma coluna 'data_ingestao' com o timestamp exato do momento da carga.
# Esta coluna √© vital para:
# - Auditoria: Saber quando um registro foi processado pela √∫ltima vez.
# - Rastreabilidade: Acompanhar a origem e o tempo de vida dos dados.
# - Time Travel do Delta Lake: Permite consultas a vers√µes espec√≠ficas dos dados baseadas no tempo.
df_enriched = df_clean.withColumn("data_ingestao", current_timestamp())

# ----------------------------------------------------------------------------------------------------------------------
# ‚úÖ 6. Reparticionamento para otimiza√ß√£o de escrita e leitura
#    (Estrat√©gia de performance para opera√ß√µes futuras)
# ----------------------------------------------------------------------------------------------------------------------
# Reparticiona o DataFrame com base na coluna 'EstabelecimentoID'.
# - Benef√≠cios:
#   - Otimiza opera√ß√µes de escrita no Delta Lake, pois os dados com o mesmo ID s√£o agrupados.
#   - Melhora o desempenho de consultas futuras que filtram ou fazem join por 'EstabelecimentoID'.
#   - Reduz o volume de dados a serem embaralhados (shuffle) em opera√ß√µes subsequentes.
# - Considera√ß√µes: 'repartition' for√ßa um shuffle completo dos dados. O n√∫mero de parti√ß√µes padr√£o
#   ser√° o n√∫mero de cores do cluster ou o valor de spark.sql.shuffle.partitions.
#   Para datasets muito grandes, pode ser necess√°rio especificar o n√∫mero de parti√ß√µes.
df_partitioned = df_enriched.repartition("EstabelecimentoID")

# ----------------------------------------------------------------------------------------------------------------------
# ‚úÖ 7. Cria√ß√£o do banco de dados l√≥gico Bronze, se ainda n√£o existir
#    (Organiza√ß√£o do Metastore para acesso via SQL)
# ----------------------------------------------------------------------------------------------------------------------
# Garante que o banco de dados 'bronze' exista no cat√°logo do Spark (Metastore).
# Isso √© um pr√©-requisito para registrar tabelas dentro dele e consult√°-las por nome qualificado (ex: bronze.estabelecimentos).
spark.sql("CREATE DATABASE IF NOT EXISTS bronze")

# ----------------------------------------------------------------------------------------------------------------------
# ‚úÖ 8. Registro da Tabela Delta no Cat√°logo (CREATE TABLE com LOCATION)
#    (Passo CRUCIAL: Garante que a tabela esteja sempre vis√≠vel para comandos SQL)
# ----------------------------------------------------------------------------------------------------------------------
# Este comando √© executado ANTES do bloco IF/ELSE de escrita.
# Isso garante que a tabela l√≥gica 'bronze.estabelecimentos' esteja sempre registrada no Metastore,
# independentemente de ser a primeira execu√ß√£o ou uma subsequente.
# Se a tabela j√° existe fisicamente mas foi desregistrada, este comando a reconecta.
# Isso resolve o erro "TABLE_OR_VIEW_NOT_FOUND" para opera√ß√µes SQL como OPTIMIZE e VACUUM.
spark.sql(f"""
    CREATE TABLE IF NOT EXISTS {TABLE_NAME}
    USING DELTA
    LOCATION '{BRONZE_TABLE_PATH}'
""")

# ----------------------------------------------------------------------------------------------------------------------
# ‚úÖ 9. L√≥gica de Escrita Delta: MERGE INTO para UPSERT ou Cria√ß√£o Inicial
#    (Abordagem idempotente e eficiente para a camada Bronze)
# ----------------------------------------------------------------------------------------------------------------------
# Verifica se o caminho Delta j√° cont√©m uma tabela Delta.
if DeltaTable.isDeltaTable(spark, BRONZE_TABLE_PATH):
    # üîÅ Cen√°rio: A tabela Delta j√° existe fisicamente. Realiza um UPSERT (MERGE INTO).
    # O MERGE INTO √© a opera√ß√£o preferida para cargas incrementais na Bronze, pois:
    # - √â at√¥mico e garante as propriedades ACID (Atomicidade, Consist√™ncia, Isolamento, Durabilidade).
    # - Permite atualizar registros existentes e inserir novos em uma √∫nica transa√ß√£o.
    # - √â eficiente, pois processa apenas as mudan√ßas.
    delta_table = DeltaTable.forPath(spark, BRONZE_TABLE_PATH) # Instancia um objeto DeltaTable para a tabela alvo.

    (
        delta_table.alias("tgt") # Define um alias 'tgt' (target) para a tabela Delta existente.
        .merge(
            df_partitioned.alias("src"), # Define um alias 'src' (source) para o DataFrame de entrada.
            "tgt.EstabelecimentoID = src.EstabelecimentoID" # Condi√ß√£o de JOIN para identificar registros correspondentes.
                                                            # 'EstabelecimentoID' √© a chave prim√°ria para o UPSERT.
        )
        .whenMatchedUpdate(set={ # Regra para quando um registro na fonte (src) corresponde a um no alvo (tgt).
            "Local": "src.Local", # Atualiza a coluna 'Local' com o valor da fonte.
            "Email": "src.Email", # Atualiza a coluna 'Email' com o valor da fonte.
            "Telefone": "src.Telefone", # Atualiza a coluna 'Telefone' com o valor da fonte.
            "data_ingestao": "src.data_ingestao" # Atualiza a data de ingest√£o para refletir a √∫ltima modifica√ß√£o.
                                                # Isso √© crucial para o Time Travel e auditoria de atualiza√ß√£o.
        })
        .whenNotMatchedInsert(values={ # Regra para quando um registro na fonte (src) N√ÉO corresponde a nenhum no alvo (tgt).
            "EstabelecimentoID": "src.EstabelecimentoID", # Insere o 'EstabelecimentoID' do novo registro.
            "Local": "src.Local", # Insere o 'Local' do novo registro.
            "Email": "src.Email", # Insere o 'Email' do novo registro.
            "Telefone": "src.Telefone", # Insere o 'Telefone' do novo registro.
            "data_ingestao": "src.data_ingestao" # Insere a data de ingest√£o para o novo registro.
        })
        .execute() # Executa a opera√ß√£o MERGE INTO.
    )
else:
    # üÜï Cen√°rio: A tabela Delta N√ÉO existe fisicamente. Realiza a cria√ß√£o inicial.
    # Esta √© a primeira carga de dados para o caminho da Bronze.
    (
        df_partitioned.write # Inicia a opera√ß√£o de escrita do DataFrame.
            .format("delta") # Especifica o formato de sa√≠da como Delta Lake.
            .partitionBy("data_ingestao") # Particiona fisicamente os dados no sistema de arquivos por 'data_ingestao'.
                                          # Isso otimiza consultas que filtram por data/per√≠odo, comum em Data Lakes.
            .option("mergeSchema", "true") # Habilita a evolu√ß√£o autom√°tica de schema.
                                           # Permite que novas colunas na fonte sejam adicionadas automaticamente
                                           # √† tabela Delta sem falhar o pipeline. Essencial para a flexibilidade da Bronze.
            .mode("overwrite") # Define o modo de escrita como 'overwrite'.
                               # Para a primeira carga, 'overwrite' garante um estado limpo e inicial da tabela.
                               # Se a inten√ß√£o fosse sempre adicionar (mesmo na primeira carga), 'append' seria usado,
                               # mas 'overwrite' √© mais seguro para garantir a integridade do primeiro snapshot.
            .save(BRONZE_TABLE_PATH) # Salva o DataFrame como uma tabela Delta no caminho especificado.
    )

# ----------------------------------------------------------------------------------------------------------------------
# ‚úÖ 10. Otimiza√ß√µes do Delta Lake
#    (Melhoram drasticamente a performance de leitura e gerenciam o custo de armazenamento)
# ----------------------------------------------------------------------------------------------------------------------
# As opera√ß√µes de OPTIMIZE e VACUUM s√£o executadas ap√≥s o registro da tabela no cat√°logo.

# Otimiza a tabela Delta, compactando pequenos arquivos em arquivos maiores.
# Aplica ZORDER BY na coluna 'EstabelecimentoID' para otimizar consultas que filtram por esta coluna.
spark.sql(f"""
    OPTIMIZE {TABLE_NAME}
    ZORDER BY (EstabelecimentoID)
""")

# Remove arquivos de dados antigos n√£o referenciados pela tabela Delta, liberando espa√ßo.
# Ret√©m o hist√≥rico por 168 horas (7 dias) para Time Travel.
spark.sql(f"""
    VACUUM {TABLE_NAME} RETAIN 168 HOURS
""")

# ----------------------------------------------------------------------------------------------------------------------
# ‚úÖ 11. Log t√©cnico de auditoria
#    (Confirma√ß√£o visual do sucesso do pipeline)
# ----------------------------------------------------------------------------------------------------------------------
print("‚úÖ Pipeline Bronze finalizado com sucesso! Tabela dispon√≠vel em:", TABLE_NAME)


# COMMAND ----------

# MAGIC %sql
# MAGIC SELECT * FROM bronze.estabelecimentos LIMIT 10

# COMMAND ----------

# üìã Registro de Log de Execu√ß√£o Delta (com diagn√≥stico embutido)
# üß† Objetivo: capturar execu√ß√£o de pipelines com rastreabilidade robusta e confi√°vel

from pyspark.sql.functions import current_timestamp
from pyspark.sql.types import StructType, StructField, StringType, TimestampType, IntegerType, DoubleType
import time

# ------------------------------------------
# ‚úÖ 1. In√≠cio da medi√ß√£o de tempo
# ------------------------------------------
start_time = time.time()

# ------------------------------------------
# ‚úÖ 2. Nome l√≥gico da execu√ß√£o (ajuste conforme o pipeline)
# ------------------------------------------
job_name = "bronze_estabelecimentos"

try:
    # ------------------------------------------
    # ‚úÖ 3. Simula√ß√£o de processamento do pipeline (substitua pelo seu df real)
    # Exemplo: leitura, transforma√ß√£o, escrita...
    # ------------------------------------------
    df = spark.read.option("header", "true").csv("dbfs:/FileStore/Ampev/estabelecimentos.csv")
    df = df.dropDuplicates().na.drop()
    
    # Valida√ß√£o de schema m√≠nimo
    if "EstabelecimentoID" not in df.columns:
        raise Exception("Coluna obrigat√≥ria 'EstabelecimentoID' n√£o encontrada no DataFrame.")

    # Contagem dos registros processados
    qtd_linhas = df.count()
    
    # Se chegou at√© aqui, deu tudo certo
    status = "SUCESSO"
    erro = None

except Exception as e:
    # Em caso de qualquer erro: captura detalhes
    qtd_linhas = 0
    status = "ERRO"
    erro = str(e)

# ------------------------------------------
# ‚úÖ 4. C√°lculo do tempo de execu√ß√£o
# ------------------------------------------
tempo_total = round(time.time() - start_time, 2)

# ------------------------------------------
# ‚úÖ 5. Defini√ß√£o do schema do log (tipagem expl√≠cita)
# ------------------------------------------
log_schema = StructType([
    StructField("job_name", StringType(), True),
    StructField("data_execucao", TimestampType(), True),
    StructField("qtd_linhas", IntegerType(), True),
    StructField("status", StringType(), True),
    StructField("erro", StringType(), True),
    StructField("tempo_total_segundos", DoubleType(), True)
])

# ------------------------------------------
# ‚úÖ 6. Cria√ß√£o do DataFrame de log
# ------------------------------------------
log_df = spark.createDataFrame([(
    job_name,
    None,            # Ser√° preenchido com timestamp na pr√≥xima linha
    qtd_linhas,
    status,
    erro,
    tempo_total
)], schema=log_schema).withColumn("data_execucao", current_timestamp())

# ------------------------------------------
# ‚úÖ 7. Caminhos para armazenamento do log
# ------------------------------------------
LOG_PATH = "dbfs:/FileStore/Ampev/logs/bronze_estabelecimentos"
LOG_TABLE = "bronze.logs_estabelecimentos"

# ------------------------------------------
# ‚úÖ 8. Escrita segura em Delta Lake
# - Cria tabela se n√£o existir
# - Permite evolu√ß√£o de schema
# ------------------------------------------
log_df.write.format("delta") \
    .mode("append") \
    .option("mergeSchema", "true") \
    .save(LOG_PATH)

spark.sql(f"""
    CREATE TABLE IF NOT EXISTS {LOG_TABLE}
    USING DELTA
    LOCATION '{LOG_PATH}'
""")

# ------------------------------------------
# ‚úÖ 9. Confirma√ß√£o visual no notebook
# ------------------------------------------
print("‚úÖ Log de execu√ß√£o registrado com sucesso!")
print(f"üìå Job: {job_name}")
print(f"üì¶ Status: {status} | Registros: {qtd_linhas} | Dura√ß√£o: {tempo_total} segundos")
if erro:
    print(f"‚ö†Ô∏è Detalhe do erro: {erro}")


# COMMAND ----------

# MAGIC %sql
# MAGIC SELECT * FROM bronze.logs_estabelecimentos