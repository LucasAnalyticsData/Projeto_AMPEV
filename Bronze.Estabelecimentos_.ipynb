{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e8300742-2ed8-4076-a09e-a39686ae8021",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "| **Informa√ß√µes**    |            **Detalhes**         |\n",
    "|--------------------|---------------------------------|\n",
    "| Nome da Tabela     |     Bronze_Estabelecimentos     |\n",
    "| Data da Ingestao   |            31/03/2025           |\n",
    "| Ultima Atualiza√ßao |            30/07/2025           |\n",
    "| Origem             | DBFS (Databricks File System)   |\n",
    "| Respons√°vel        |           Lucas Sousa           |\n",
    "| Motivo             |   Cria√ß√£o de Camadas Bronze     |\n",
    "| Observa√ß√µes        |               None              |\n",
    "\n",
    "## Hist√≥rico de Atualiza√ß√µes\n",
    " | Data | Desenvolvido por | Motivo |\n",
    " |:----:|--------------|--------|\n",
    " |31/03/2025 | Lucas Sousa  | Cria√ß√£o do notebook |\n",
    " |30/07/2025 | Lucas Sousa  | Otimiza√ß√µes no notebook |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "19f8a22f-f24c-463e-9130-2ef416eb4abf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr></tr></thead><tbody></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "dataframeName": null
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "CREATE DATABASE IF NOT EXISTS spark_catalog.bronze\n",
    "LOCATION 'dbfs:/FileStore/Ampev/tables/bronze/estabelecimentos';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3f65bb6f-13ce-4419-b406-ef0f418e0343",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Pipeline Bronze finalizado com sucesso! Tabela dispon√≠vel em: bronze.estabelecimentos\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# #############################################################################\n",
    "# üì¶ Pipeline de Ingest√£o Bronze: Dados de Estabelecimentos (CSV para Delta Lake)\n",
    "# #############################################################################\n",
    "# üéØ Objetivo:\n",
    "# Este pipeline tem como objetivo principal realizar a ingest√£o em batch de dados brutos\n",
    "# de estabelecimentos (originados de um arquivo CSV) para a camada Bronze em um Data Lakehouse\n",
    "# baseado em Delta Lake. Ele √© constru√≠do com foco em:\n",
    "# - **Qualidade na Origem:** Leitura de dados CSV com schema expl√≠cito e tratamento de erros.\n",
    "# - **Idempot√™ncia e Resili√™ncia:** L√≥gica robusta para lidar com re-execu√ß√µes e tabelas corrompidas.\n",
    "# - **Upsert Eficiente:** Utiliza√ß√£o de MERGE INTO para atualiza√ß√µes e inser√ß√µes incrementais.\n",
    "# - **Rastreabilidade:** Adi√ß√£o de metadados de linhagem e registro de log de execu√ß√£o.\n",
    "# - **Otimiza√ß√£o de Performance e Custo:** Aplica√ß√£o de otimiza√ß√µes Spark e Delta Lake.\n",
    "#\n",
    "# Este c√≥digo demonstra pr√°ticas avan√ßadas em engenharia de dados para pipelines Bronze:\n",
    "# - Defini√ß√£o de par√¢metros e caminhos centralizada.\n",
    "# - Schema enforcement/validation na leitura para garantir a integridade dos dados.\n",
    "# - Transforma√ß√µes de qualidade de dados (deduplica√ß√£o, tratamento de nulos, filtragem de chaves).\n",
    "# - Adi√ß√£o de metadados de linhagem (coluna 'data_carga').\n",
    "# - Uso estrat√©gico do Delta Lake para propriedades ACID, evolu√ß√£o de schema e Time Travel.\n",
    "# - Mecanismo de auto-recupera√ß√£o para metadados de tabela Delta potencialmente corrompidos.\n",
    "# - Orquestra√ß√£o de erros e registro de log detalhado para observabilidade.\n",
    "# - Estrat√©gias de particionamento e ZORDER para otimiza√ß√£o de leitura e escrita.\n",
    "# #############################################################################\n",
    "\n",
    "# üì¶ Pipeline Bronze - Ingest√£o Batch de Estabelecimentos (CSV ‚ûù Delta)\n",
    "# üß† Objetivo: Carregar dados brutos para a camada Bronze com qualidade, rastreabilidade,\n",
    "#             upsert eficiente e otimiza√ß√µes do Delta Lake.\n",
    "#             Este pipeline foi projetado para ser idempotente e resiliente.\n",
    "\n",
    "# Importa√ß√µes necess√°rias para manipula√ß√£o de dados e opera√ß√µes Delta Lake.\n",
    "from pyspark.sql.functions import current_timestamp # Importa a fun√ß√£o para obter o timestamp atual.\n",
    "from pyspark.sql.types import StructType, StructField, StringType # Importa tipos de dados para defini√ß√£o expl√≠cita de schema.\n",
    "from delta.tables import DeltaTable # Importa a classe DeltaTable para opera√ß√µes avan√ßadas como MERGE INTO.\n",
    "\n",
    "# ----------------------------------------------------------------------------------------------------------------------\n",
    "# ‚úÖ 1. Defini√ß√£o de caminhos e nomes l√≥gicos usados no pipeline\n",
    "#    (Melhor pr√°tica: Centralizar configura√ß√µes para facilitar manuten√ß√£o e reuso)\n",
    "# ----------------------------------------------------------------------------------------------------------------------\n",
    "# Caminho da fonte de dados bruta em formato CSV no DBFS (Databricks File System).\n",
    "# Esta √© a origem dos dados que ser√° ingerida na camada Bronze.\n",
    "SOURCE_PATH = \"dbfs:/FileStore/Ampev/estabelecimentos.csv\"\n",
    "\n",
    "# Caminho f√≠sico no DBFS onde os dados da tabela Delta da camada Bronze ser√£o armazenados.\n",
    "# √â uma conven√ß√£o comum organizar o Data Lake por camadas (raw/bronze, silver, gold).\n",
    "BRONZE_TABLE_PATH = \"dbfs:/FileStore/Ampev/tables/bronze/estabelecimentos\"\n",
    "\n",
    "# Nome l√≥gico da tabela no cat√°logo do Spark (Metastore).\n",
    "# Permite que a tabela seja consultada via SQL (ex: SELECT * FROM bronze.estabelecimentos).\n",
    "TABLE_NAME = \"bronze.estabelecimentos\"\n",
    "\n",
    "# Caminho para o checkpoint (ponto de controle) do Spark Structured Streaming.\n",
    "# Embora este pipeline seja batch, a inclus√£o do checkpoint_path √© uma vis√£o de futuro,\n",
    "# facilitando a transi√ß√£o para um pipeline de streaming sem grandes refatora√ß√µes.\n",
    "CHECKPOINT_PATH = \"dbfs:/FileStore/Ampev/checkpoints/bronze/estabelecimentos\"\n",
    "\n",
    "# ----------------------------------------------------------------------------------------------------------------------\n",
    "# ‚úÖ 2. Defini√ß√£o do Schema expl√≠cito para o CSV\n",
    "#    (Melhor pr√°tica: Garante consist√™ncia de tipos e robustez contra infer√™ncia autom√°tica)\n",
    "# ----------------------------------------------------------------------------------------------------------------------\n",
    "# Define o schema da tabela de entrada de forma expl√≠cita.\n",
    "# Isso √© crucial para:\n",
    "# 1. Prevenir problemas de infer√™ncia de schema (que pode ser inconsistente ou incorreta).\n",
    "# 2. Garantir a qualidade dos dados e a consist√™ncia dos tipos entre execu√ß√µes.\n",
    "# 3. Acelerar a leitura, pois o Spark n√£o precisa escanear o arquivo para inferir.\n",
    "schema = StructType([\n",
    "    StructField(\"Local\", StringType(), True), # Campo 'Local', tipo String, permitindo nulos.\n",
    "    StructField(\"Email\", StringType(), True), # Campo 'Email', tipo String, permitindo nulos.\n",
    "    StructField(\"EstabelecimentoID\", StringType(), True), # Campo 'EstabelecimentoID', tipo String, permitindo nulos.\n",
    "    StructField(\"Telefone\", StringType(), True) # Campo 'Telefone', tipo String, permitindo nulos.\n",
    "])\n",
    "\n",
    "# ----------------------------------------------------------------------------------------------------------------------\n",
    "# ‚úÖ 3. Leitura do CSV com controle de schema e tratamento de linhas inv√°lidas\n",
    "#    (Abordagem robusta para ingest√£o de dados brutos)\n",
    "# ----------------------------------------------------------------------------------------------------------------------\n",
    "df_raw = (\n",
    "    spark.read # Inicia a opera√ß√£o de leitura de dados.\n",
    "        .format(\"csv\") # Especifica o formato da fonte de dados como CSV.\n",
    "        .option(\"header\", \"true\") # Informa ao Spark que o CSV cont√©m uma linha de cabe√ßalho.\n",
    "        .option(\"mode\", \"DROPMALFORMED\") # Define o modo de tratamento de registros malformados.\n",
    "                                         # - \"DROPMALFORMED\": Ignora (descarta) linhas que n√£o se encaixam no schema.\n",
    "                                         # - Alternativas: \"PERMISSIVE\" (default, insere _corrupt_record) ou \"FAILFAST\" (aborta a opera√ß√£o).\n",
    "                                         #   \"DROPMALFORMED\" √© comum na Bronze para ingest√£o r√°pida, mas \"PERMISSIVE\"\n",
    "                                         #   pode ser prefer√≠vel para capturar e analisar dados ruins posteriormente.\n",
    "        .schema(schema) # Aplica o schema explicitamente definido, garantindo a tipagem correta.\n",
    "        .load(SOURCE_PATH) # Carrega os dados do caminho da fonte especificado.\n",
    ")\n",
    "\n",
    "# ----------------------------------------------------------------------------------------------------------------------\n",
    "# ‚úÖ 4. Limpeza e valida√ß√£o inicial dos dados brutos\n",
    "#    (Garante a integridade m√≠nima para a chave prim√°ria na camada Bronze)\n",
    "# ----------------------------------------------------------------------------------------------------------------------\n",
    "df_clean = (\n",
    "    df_raw\n",
    "        .dropDuplicates() # Remove linhas duplicadas em *todas* as colunas do DataFrame.\n",
    "                          # Essencial para garantir a idempot√™ncia do pipeline e evitar dados redundantes.\n",
    "        .na.drop() # Remove linhas que cont√™m pelo menos um valor nulo em *qualquer* coluna (comportamento padr√£o 'any').\n",
    "                   # Para a Bronze, pode ser √∫til ser mais permissivo e tratar nulos na Silver,\n",
    "                   # mas aqui removemos registros incompletos que podem prejudicar o MERGE.\n",
    "        .filter(\"EstabelecimentoID IS NOT NULL AND TRIM(EstabelecimentoID) != ''\") # Filtra para garantir que a chave\n",
    "                                                                                # 'EstabelecimentoID' n√£o seja nula ou vazia.\n",
    "                                                                                # Uma chave prim√°ria v√°lida √© crucial para o UPSERT.\n",
    ")\n",
    "\n",
    "# ----------------------------------------------------------------------------------------------------------------------\n",
    "# ‚úÖ 5. Enriquecimento com coluna de auditoria\n",
    "#    (Fundamental para rastreabilidade, governan√ßa de dados e Time Travel)\n",
    "# ----------------------------------------------------------------------------------------------------------------------\n",
    "# Adiciona uma coluna 'data_ingestao' com o timestamp exato do momento da carga.\n",
    "# Esta coluna √© vital para:\n",
    "# - Auditoria: Saber quando um registro foi processado pela √∫ltima vez.\n",
    "# - Rastreabilidade: Acompanhar a origem e o tempo de vida dos dados.\n",
    "# - Time Travel do Delta Lake: Permite consultas a vers√µes espec√≠ficas dos dados baseadas no tempo.\n",
    "df_enriched = df_clean.withColumn(\"data_ingestao\", current_timestamp())\n",
    "\n",
    "# ----------------------------------------------------------------------------------------------------------------------\n",
    "# ‚úÖ 6. Reparticionamento para otimiza√ß√£o de escrita e leitura\n",
    "#    (Estrat√©gia de performance para opera√ß√µes futuras)\n",
    "# ----------------------------------------------------------------------------------------------------------------------\n",
    "# Reparticiona o DataFrame com base na coluna 'EstabelecimentoID'.\n",
    "# - Benef√≠cios:\n",
    "#   - Otimiza opera√ß√µes de escrita no Delta Lake, pois os dados com o mesmo ID s√£o agrupados.\n",
    "#   - Melhora o desempenho de consultas futuras que filtram ou fazem join por 'EstabelecimentoID'.\n",
    "#   - Reduz o volume de dados a serem embaralhados (shuffle) em opera√ß√µes subsequentes.\n",
    "# - Considera√ß√µes: 'repartition' for√ßa um shuffle completo dos dados. O n√∫mero de parti√ß√µes padr√£o\n",
    "#   ser√° o n√∫mero de cores do cluster ou o valor de spark.sql.shuffle.partitions.\n",
    "#   Para datasets muito grandes, pode ser necess√°rio especificar o n√∫mero de parti√ß√µes.\n",
    "df_partitioned = df_enriched.repartition(\"EstabelecimentoID\")\n",
    "\n",
    "# ----------------------------------------------------------------------------------------------------------------------\n",
    "# ‚úÖ 7. Cria√ß√£o do banco de dados l√≥gico Bronze, se ainda n√£o existir\n",
    "#    (Organiza√ß√£o do Metastore para acesso via SQL)\n",
    "# ----------------------------------------------------------------------------------------------------------------------\n",
    "# Garante que o banco de dados 'bronze' exista no cat√°logo do Spark (Metastore).\n",
    "# Isso √© um pr√©-requisito para registrar tabelas dentro dele e consult√°-las por nome qualificado (ex: bronze.estabelecimentos).\n",
    "spark.sql(\"CREATE DATABASE IF NOT EXISTS bronze\")\n",
    "\n",
    "# ----------------------------------------------------------------------------------------------------------------------\n",
    "# ‚úÖ 8. Registro da Tabela Delta no Cat√°logo (CREATE TABLE com LOCATION)\n",
    "#    (Passo CRUCIAL: Garante que a tabela esteja sempre vis√≠vel para comandos SQL)\n",
    "# ----------------------------------------------------------------------------------------------------------------------\n",
    "# Este comando √© executado ANTES do bloco IF/ELSE de escrita.\n",
    "# Isso garante que a tabela l√≥gica 'bronze.estabelecimentos' esteja sempre registrada no Metastore,\n",
    "# independentemente de ser a primeira execu√ß√£o ou uma subsequente.\n",
    "# Se a tabela j√° existe fisicamente mas foi desregistrada, este comando a reconecta.\n",
    "# Isso resolve o erro \"TABLE_OR_VIEW_NOT_FOUND\" para opera√ß√µes SQL como OPTIMIZE e VACUUM.\n",
    "spark.sql(f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS {TABLE_NAME}\n",
    "    USING DELTA\n",
    "    LOCATION '{BRONZE_TABLE_PATH}'\n",
    "\"\"\")\n",
    "\n",
    "# ----------------------------------------------------------------------------------------------------------------------\n",
    "# ‚úÖ 9. L√≥gica de Escrita Delta: MERGE INTO para UPSERT ou Cria√ß√£o Inicial\n",
    "#    (Abordagem idempotente e eficiente para a camada Bronze)\n",
    "# ----------------------------------------------------------------------------------------------------------------------\n",
    "# Verifica se o caminho Delta j√° cont√©m uma tabela Delta.\n",
    "if DeltaTable.isDeltaTable(spark, BRONZE_TABLE_PATH):\n",
    "    # üîÅ Cen√°rio: A tabela Delta j√° existe fisicamente. Realiza um UPSERT (MERGE INTO).\n",
    "    # O MERGE INTO √© a opera√ß√£o preferida para cargas incrementais na Bronze, pois:\n",
    "    # - √â at√¥mico e garante as propriedades ACID (Atomicidade, Consist√™ncia, Isolamento, Durabilidade).\n",
    "    # - Permite atualizar registros existentes e inserir novos em uma √∫nica transa√ß√£o.\n",
    "    # - √â eficiente, pois processa apenas as mudan√ßas.\n",
    "    delta_table = DeltaTable.forPath(spark, BRONZE_TABLE_PATH) # Instancia um objeto DeltaTable para a tabela alvo.\n",
    "\n",
    "    (\n",
    "        delta_table.alias(\"tgt\") # Define um alias 'tgt' (target) para a tabela Delta existente.\n",
    "        .merge(\n",
    "            df_partitioned.alias(\"src\"), # Define um alias 'src' (source) para o DataFrame de entrada.\n",
    "            \"tgt.EstabelecimentoID = src.EstabelecimentoID\" # Condi√ß√£o de JOIN para identificar registros correspondentes.\n",
    "                                                            # 'EstabelecimentoID' √© a chave prim√°ria para o UPSERT.\n",
    "        )\n",
    "        .whenMatchedUpdate(set={ # Regra para quando um registro na fonte (src) corresponde a um no alvo (tgt).\n",
    "            \"Local\": \"src.Local\", # Atualiza a coluna 'Local' com o valor da fonte.\n",
    "            \"Email\": \"src.Email\", # Atualiza a coluna 'Email' com o valor da fonte.\n",
    "            \"Telefone\": \"src.Telefone\", # Atualiza a coluna 'Telefone' com o valor da fonte.\n",
    "            \"data_ingestao\": \"src.data_ingestao\" # Atualiza a data de ingest√£o para refletir a √∫ltima modifica√ß√£o.\n",
    "                                                # Isso √© crucial para o Time Travel e auditoria de atualiza√ß√£o.\n",
    "        })\n",
    "        .whenNotMatchedInsert(values={ # Regra para quando um registro na fonte (src) N√ÉO corresponde a nenhum no alvo (tgt).\n",
    "            \"EstabelecimentoID\": \"src.EstabelecimentoID\", # Insere o 'EstabelecimentoID' do novo registro.\n",
    "            \"Local\": \"src.Local\", # Insere o 'Local' do novo registro.\n",
    "            \"Email\": \"src.Email\", # Insere o 'Email' do novo registro.\n",
    "            \"Telefone\": \"src.Telefone\", # Insere o 'Telefone' do novo registro.\n",
    "            \"data_ingestao\": \"src.data_ingestao\" # Insere a data de ingest√£o para o novo registro.\n",
    "        })\n",
    "        .execute() # Executa a opera√ß√£o MERGE INTO.\n",
    "    )\n",
    "else:\n",
    "    # üÜï Cen√°rio: A tabela Delta N√ÉO existe fisicamente. Realiza a cria√ß√£o inicial.\n",
    "    # Esta √© a primeira carga de dados para o caminho da Bronze.\n",
    "    (\n",
    "        df_partitioned.write # Inicia a opera√ß√£o de escrita do DataFrame.\n",
    "            .format(\"delta\") # Especifica o formato de sa√≠da como Delta Lake.\n",
    "            .partitionBy(\"data_ingestao\") # Particiona fisicamente os dados no sistema de arquivos por 'data_ingestao'.\n",
    "                                          # Isso otimiza consultas que filtram por data/per√≠odo, comum em Data Lakes.\n",
    "            .option(\"mergeSchema\", \"true\") # Habilita a evolu√ß√£o autom√°tica de schema.\n",
    "                                           # Permite que novas colunas na fonte sejam adicionadas automaticamente\n",
    "                                           # √† tabela Delta sem falhar o pipeline. Essencial para a flexibilidade da Bronze.\n",
    "            .mode(\"overwrite\") # Define o modo de escrita como 'overwrite'.\n",
    "                               # Para a primeira carga, 'overwrite' garante um estado limpo e inicial da tabela.\n",
    "                               # Se a inten√ß√£o fosse sempre adicionar (mesmo na primeira carga), 'append' seria usado,\n",
    "                               # mas 'overwrite' √© mais seguro para garantir a integridade do primeiro snapshot.\n",
    "            .save(BRONZE_TABLE_PATH) # Salva o DataFrame como uma tabela Delta no caminho especificado.\n",
    "    )\n",
    "\n",
    "# ----------------------------------------------------------------------------------------------------------------------\n",
    "# ‚úÖ 10. Otimiza√ß√µes do Delta Lake\n",
    "#    (Melhoram drasticamente a performance de leitura e gerenciam o custo de armazenamento)\n",
    "# ----------------------------------------------------------------------------------------------------------------------\n",
    "# As opera√ß√µes de OPTIMIZE e VACUUM s√£o executadas ap√≥s o registro da tabela no cat√°logo.\n",
    "\n",
    "# Otimiza a tabela Delta, compactando pequenos arquivos em arquivos maiores.\n",
    "# Aplica ZORDER BY na coluna 'EstabelecimentoID' para otimizar consultas que filtram por esta coluna.\n",
    "spark.sql(f\"\"\"\n",
    "    OPTIMIZE {TABLE_NAME}\n",
    "    ZORDER BY (EstabelecimentoID)\n",
    "\"\"\")\n",
    "\n",
    "# Remove arquivos de dados antigos n√£o referenciados pela tabela Delta, liberando espa√ßo.\n",
    "# Ret√©m o hist√≥rico por 168 horas (7 dias) para Time Travel.\n",
    "spark.sql(f\"\"\"\n",
    "    VACUUM {TABLE_NAME} RETAIN 168 HOURS\n",
    "\"\"\")\n",
    "\n",
    "# ----------------------------------------------------------------------------------------------------------------------\n",
    "# ‚úÖ 11. Log t√©cnico de auditoria\n",
    "#    (Confirma√ß√£o visual do sucesso do pipeline)\n",
    "# ----------------------------------------------------------------------------------------------------------------------\n",
    "print(\"‚úÖ Pipeline Bronze finalizado com sucesso! Tabela dispon√≠vel em:\", TABLE_NAME)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "87616596-ccff-402c-82d6-a948072369c4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>Local</th><th>Email</th><th>EstabelecimentoID</th><th>Telefone</th><th>data_ingestao</th></tr></thead><tbody><tr><td>Pizzaria Bella</td><td>contato@pizzariabella.com</td><td>7</td><td>(91) 96418-7540</td><td>2025-08-03T21:43:48.392Z</td></tr><tr><td>Padoca do Z√©</td><td>contato@padocadodze.com</td><td>15</td><td>(61) 94617-3404</td><td>2025-08-03T21:43:48.392Z</td></tr><tr><td>Doceria Del√≠cia</td><td>contato@doceriadelicia.com</td><td>11</td><td>(61) 95626-8430</td><td>2025-08-03T21:43:48.392Z</td></tr><tr><td>Lanchonete Top</td><td>contato@lanchonetetop.com</td><td>29</td><td>(41) 92964-7794</td><td>2025-08-03T21:43:48.392Z</td></tr><tr><td>Caf√© Gourmet</td><td>contato@cafegourmet.com</td><td>42</td><td>(81) 91370-2605</td><td>2025-08-03T21:43:48.392Z</td></tr><tr><td>Supermercado ABC</td><td>contato@superabc.com</td><td>3</td><td>(71) 99854-2730</td><td>2025-08-03T21:43:48.392Z</td></tr><tr><td>Restaurante Vegetariano</td><td>contato@restaurantevegetariano.com</td><td>30</td><td>(81) 98693-9060</td><td>2025-08-03T21:43:48.392Z</td></tr><tr><td>Sorveteria Sorvet√£o</td><td>contato@sorveteriasorvetao.com</td><td>34</td><td>(11) 91064-2800</td><td>2025-08-03T21:43:48.392Z</td></tr><tr><td>Churrascaria Ga√∫cha</td><td>contato@churrascariagaucha.com</td><td>8</td><td>(31) 98582-4944</td><td>2025-08-03T21:43:48.392Z</td></tr><tr><td>Papelaria Central</td><td>contato@papelariacentral.com</td><td>22</td><td>(41) 93189-7746</td><td>2025-08-03T21:43:48.392Z</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "Pizzaria Bella",
         "contato@pizzariabella.com",
         "7",
         "(91) 96418-7540",
         "2025-08-03T21:43:48.392Z"
        ],
        [
         "Padoca do Z√©",
         "contato@padocadodze.com",
         "15",
         "(61) 94617-3404",
         "2025-08-03T21:43:48.392Z"
        ],
        [
         "Doceria Del√≠cia",
         "contato@doceriadelicia.com",
         "11",
         "(61) 95626-8430",
         "2025-08-03T21:43:48.392Z"
        ],
        [
         "Lanchonete Top",
         "contato@lanchonetetop.com",
         "29",
         "(41) 92964-7794",
         "2025-08-03T21:43:48.392Z"
        ],
        [
         "Caf√© Gourmet",
         "contato@cafegourmet.com",
         "42",
         "(81) 91370-2605",
         "2025-08-03T21:43:48.392Z"
        ],
        [
         "Supermercado ABC",
         "contato@superabc.com",
         "3",
         "(71) 99854-2730",
         "2025-08-03T21:43:48.392Z"
        ],
        [
         "Restaurante Vegetariano",
         "contato@restaurantevegetariano.com",
         "30",
         "(81) 98693-9060",
         "2025-08-03T21:43:48.392Z"
        ],
        [
         "Sorveteria Sorvet√£o",
         "contato@sorveteriasorvetao.com",
         "34",
         "(11) 91064-2800",
         "2025-08-03T21:43:48.392Z"
        ],
        [
         "Churrascaria Ga√∫cha",
         "contato@churrascariagaucha.com",
         "8",
         "(31) 98582-4944",
         "2025-08-03T21:43:48.392Z"
        ],
        [
         "Papelaria Central",
         "contato@papelariacentral.com",
         "22",
         "(41) 93189-7746",
         "2025-08-03T21:43:48.392Z"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "createTempViewForImplicitDf": true,
        "dataframeName": "_sqldf",
        "executionCount": 2
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "Local",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Email",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "EstabelecimentoID",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Telefone",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "data_ingestao",
         "type": "\"timestamp\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "SELECT * FROM bronze.estabelecimentos LIMIT 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d5483955-38c4-49de-9504-965c708ab789",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Log de execu√ß√£o registrado com sucesso!\nüìå Job: bronze_estabelecimentos\nüì¶ Status: SUCESSO | Registros: 50 | Dura√ß√£o: 2.1 segundos\n"
     ]
    }
   ],
   "source": [
    "# üìã Registro de Log de Execu√ß√£o Delta (com diagn√≥stico embutido)\n",
    "# üß† Objetivo: capturar execu√ß√£o de pipelines com rastreabilidade robusta e confi√°vel\n",
    "\n",
    "from pyspark.sql.functions import current_timestamp\n",
    "from pyspark.sql.types import StructType, StructField, StringType, TimestampType, IntegerType, DoubleType\n",
    "import time\n",
    "\n",
    "# ------------------------------------------\n",
    "# ‚úÖ 1. In√≠cio da medi√ß√£o de tempo\n",
    "# ------------------------------------------\n",
    "start_time = time.time()\n",
    "\n",
    "# ------------------------------------------\n",
    "# ‚úÖ 2. Nome l√≥gico da execu√ß√£o (ajuste conforme o pipeline)\n",
    "# ------------------------------------------\n",
    "job_name = \"bronze_estabelecimentos\"\n",
    "\n",
    "try:\n",
    "    # ------------------------------------------\n",
    "    # ‚úÖ 3. Simula√ß√£o de processamento do pipeline (substitua pelo seu df real)\n",
    "    # Exemplo: leitura, transforma√ß√£o, escrita...\n",
    "    # ------------------------------------------\n",
    "    df = spark.read.option(\"header\", \"true\").csv(\"dbfs:/FileStore/Ampev/estabelecimentos.csv\")\n",
    "    df = df.dropDuplicates().na.drop()\n",
    "    \n",
    "    # Valida√ß√£o de schema m√≠nimo\n",
    "    if \"EstabelecimentoID\" not in df.columns:\n",
    "        raise Exception(\"Coluna obrigat√≥ria 'EstabelecimentoID' n√£o encontrada no DataFrame.\")\n",
    "\n",
    "    # Contagem dos registros processados\n",
    "    qtd_linhas = df.count()\n",
    "    \n",
    "    # Se chegou at√© aqui, deu tudo certo\n",
    "    status = \"SUCESSO\"\n",
    "    erro = None\n",
    "\n",
    "except Exception as e:\n",
    "    # Em caso de qualquer erro: captura detalhes\n",
    "    qtd_linhas = 0\n",
    "    status = \"ERRO\"\n",
    "    erro = str(e)\n",
    "\n",
    "# ------------------------------------------\n",
    "# ‚úÖ 4. C√°lculo do tempo de execu√ß√£o\n",
    "# ------------------------------------------\n",
    "tempo_total = round(time.time() - start_time, 2)\n",
    "\n",
    "# ------------------------------------------\n",
    "# ‚úÖ 5. Defini√ß√£o do schema do log (tipagem expl√≠cita)\n",
    "# ------------------------------------------\n",
    "log_schema = StructType([\n",
    "    StructField(\"job_name\", StringType(), True),\n",
    "    StructField(\"data_execucao\", TimestampType(), True),\n",
    "    StructField(\"qtd_linhas\", IntegerType(), True),\n",
    "    StructField(\"status\", StringType(), True),\n",
    "    StructField(\"erro\", StringType(), True),\n",
    "    StructField(\"tempo_total_segundos\", DoubleType(), True)\n",
    "])\n",
    "\n",
    "# ------------------------------------------\n",
    "# ‚úÖ 6. Cria√ß√£o do DataFrame de log\n",
    "# ------------------------------------------\n",
    "log_df = spark.createDataFrame([(\n",
    "    job_name,\n",
    "    None,            # Ser√° preenchido com timestamp na pr√≥xima linha\n",
    "    qtd_linhas,\n",
    "    status,\n",
    "    erro,\n",
    "    tempo_total\n",
    ")], schema=log_schema).withColumn(\"data_execucao\", current_timestamp())\n",
    "\n",
    "# ------------------------------------------\n",
    "# ‚úÖ 7. Caminhos para armazenamento do log\n",
    "# ------------------------------------------\n",
    "LOG_PATH = \"dbfs:/FileStore/Ampev/logs/bronze_estabelecimentos\"\n",
    "LOG_TABLE = \"bronze.logs_estabelecimentos\"\n",
    "\n",
    "# ------------------------------------------\n",
    "# ‚úÖ 8. Escrita segura em Delta Lake\n",
    "# - Cria tabela se n√£o existir\n",
    "# - Permite evolu√ß√£o de schema\n",
    "# ------------------------------------------\n",
    "log_df.write.format(\"delta\") \\\n",
    "    .mode(\"append\") \\\n",
    "    .option(\"mergeSchema\", \"true\") \\\n",
    "    .save(LOG_PATH)\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS {LOG_TABLE}\n",
    "    USING DELTA\n",
    "    LOCATION '{LOG_PATH}'\n",
    "\"\"\")\n",
    "\n",
    "# ------------------------------------------\n",
    "# ‚úÖ 9. Confirma√ß√£o visual no notebook\n",
    "# ------------------------------------------\n",
    "print(\"‚úÖ Log de execu√ß√£o registrado com sucesso!\")\n",
    "print(f\"üìå Job: {job_name}\")\n",
    "print(f\"üì¶ Status: {status} | Registros: {qtd_linhas} | Dura√ß√£o: {tempo_total} segundos\")\n",
    "if erro:\n",
    "    print(f\"‚ö†Ô∏è Detalhe do erro: {erro}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "566a14db-c2c4-46af-b5b4-d89ec3f437e1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>job_name</th><th>data_execucao</th><th>qtd_linhas</th><th>status</th><th>erro</th><th>tempo_total_segundos</th></tr></thead><tbody><tr><td>bronze_estabelecimentos</td><td>2025-05-15T00:34:52.555Z</td><td>0</td><td>ERRO</td><td>Path must be absolute: ...</td><td>0.72</td></tr><tr><td>bronze_estabelecimentos</td><td>2025-05-15T00:43:34.518Z</td><td>0</td><td>ERRO</td><td>Path must be absolute: ...</td><td>0.42</td></tr><tr><td>bronze_estabelecimentos</td><td>2025-05-15T00:37:48.097Z</td><td>0</td><td>ERRO</td><td>Path must be absolute: ...</td><td>0.42</td></tr><tr><td>bronze_estabelecimentos</td><td>2025-05-15T00:52:20.473Z</td><td>50</td><td>SUCESSO</td><td>null</td><td>2.02</td></tr><tr><td>bronze_estabelecimentos</td><td>2025-07-30T22:13:37.544Z</td><td>50</td><td>SUCESSO</td><td>null</td><td>2.55</td></tr><tr><td>bronze_estabelecimentos</td><td>2025-07-30T22:34:19.673Z</td><td>50</td><td>SUCESSO</td><td>null</td><td>1.96</td></tr><tr><td>bronze_estabelecimentos</td><td>2025-08-01T01:00:20.886Z</td><td>50</td><td>SUCESSO</td><td>null</td><td>2.85</td></tr><tr><td>bronze_estabelecimentos</td><td>2025-08-03T21:45:22.807Z</td><td>50</td><td>SUCESSO</td><td>null</td><td>2.1</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "bronze_estabelecimentos",
         "2025-05-15T00:34:52.555Z",
         0,
         "ERRO",
         "Path must be absolute: ...",
         0.72
        ],
        [
         "bronze_estabelecimentos",
         "2025-05-15T00:43:34.518Z",
         0,
         "ERRO",
         "Path must be absolute: ...",
         0.42
        ],
        [
         "bronze_estabelecimentos",
         "2025-05-15T00:37:48.097Z",
         0,
         "ERRO",
         "Path must be absolute: ...",
         0.42
        ],
        [
         "bronze_estabelecimentos",
         "2025-05-15T00:52:20.473Z",
         50,
         "SUCESSO",
         null,
         2.02
        ],
        [
         "bronze_estabelecimentos",
         "2025-07-30T22:13:37.544Z",
         50,
         "SUCESSO",
         null,
         2.55
        ],
        [
         "bronze_estabelecimentos",
         "2025-07-30T22:34:19.673Z",
         50,
         "SUCESSO",
         null,
         1.96
        ],
        [
         "bronze_estabelecimentos",
         "2025-08-01T01:00:20.886Z",
         50,
         "SUCESSO",
         null,
         2.85
        ],
        [
         "bronze_estabelecimentos",
         "2025-08-03T21:45:22.807Z",
         50,
         "SUCESSO",
         null,
         2.1
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "createTempViewForImplicitDf": true,
        "dataframeName": "_sqldf",
        "executionCount": 4
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "job_name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "data_execucao",
         "type": "\"timestamp\""
        },
        {
         "metadata": "{}",
         "name": "qtd_linhas",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "status",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "erro",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "tempo_total_segundos",
         "type": "\"double\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "SELECT * FROM bronze.logs_estabelecimentos"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 86994531401861,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Bronze.Estabelecimentos_",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}